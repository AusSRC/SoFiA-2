{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hercules-3.nimbus.pawsey.org.au:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hercules-3.nimbus.pawsey.org.au:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe5f11f5978>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatemod7(val):\n",
    "    sys.path.append('example')\n",
    "    import example\n",
    "    return example.my_mod(val, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n"
     ]
    }
   ],
   "source": [
    "#import example\n",
    "import myLib\n",
    "\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random as rnd\n",
    "from time import sleep\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark import StorageLevel\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import randn, monotonically_increasing_id, row_number, desc, udf, col, lit\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType,Row, BooleanType, StringType\n",
    "from pyspark.sql.types import MapType, StructType,StructField\n",
    "from myLib import GetDetailArrays,GetSubCube2,GetDataframeSize,CreateFITSSubCubeUDF, FlattenDataFrame \n",
    "\"\"\"\n",
    "Set the Spark datatypes\n",
    "\"\"\"\n",
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector,VectorUDT\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time as tm\n",
    "import logging\n",
    "from time import time, clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use fits_investigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|partition                                                      |\n",
      "+---------------------------------------------------------------+\n",
      "|spi_filename=__HIVE_DEFAULT_PARTITION__/spi_band=1             |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=0   |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1   |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=10  |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=100 |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1000|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1001|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1002|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1003|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1004|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1005|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1006|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1007|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1008|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1009|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=101 |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1010|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1011|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1012|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1013|\n",
      "+---------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show partitions sparkfits_images\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|spi_index|\n",
      "+---------+\n",
      "|        0|\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "|       10|\n",
      "|       11|\n",
      "|       12|\n",
      "|       13|\n",
      "|       14|\n",
      "|       15|\n",
      "|       16|\n",
      "|       17|\n",
      "|       18|\n",
      "|       19|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    select spi_index from sparkfits_images\n",
    "    where spi_filename='image.restored.i.SB2338.V2.cube.fits'\n",
    "    and spi_band=24    \n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partitioner:\n",
    "    def __init__(self):\n",
    "        self.callPerDriverSetup\n",
    "        \n",
    "    def callPerDriverSetup(self):\n",
    "        pass\n",
    "    \n",
    "    def callPerPartitionSetup(self):\n",
    "        sys.path.append('example')                  ### <=== Either append or use the --pyfiles parameter\n",
    "        import example\n",
    "         \n",
    "        self.example=example\n",
    "        self.parameterFile=SparkFiles.get('default_parameters.par')\n",
    "        \n",
    "    def doProcess(self, element):\n",
    "        ###  here's the call to the C library for each row of the dataframe partition\n",
    "        ### In here we have to transform the flattened array data to the format SoFiA\n",
    "        ### requires, as well as the\n",
    "        print(\"Hi there\")\n",
    "        \n",
    "        ra=np.array(element.rightAscension, dtype='<f4')\n",
    "        Pixels=np.array(element.pixels, dtype='<f4')\n",
    "        Pixels=Pixels.reshape(Pixels.shape[0], Pixels.shape[1], Pixels.shape[2], Pixels.shape[4])\n",
    "        \n",
    "        mm=self.example.my_mod(element.bins, 7)\n",
    "        lines=0\n",
    "        with open(self.parameterFile) as f:\n",
    "            for line in f:\n",
    "                lines = lines + 1\n",
    "        \n",
    "        msg=\"{} {}\".format(str(mm), str(lines))\n",
    "        \n",
    "        return msg\n",
    "        ##return self.example.my_mod(element.bins, 7)\n",
    "        ## return self.example.my_mod(element.wire, 7) \n",
    "    \n",
    "    def processPartition(self, partition):\n",
    "        self.callPerPartitionSetup()\n",
    "        for element in partition:\n",
    "            yield self.doProcess(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the SoFiA parameter to the spark classpath\n",
    "\n",
    "We do this so the parameter file cam bepassed to the SoFiA executable within the partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('hdfs:///user/hduser/ConfigurationFiles/SoFiA/default_parameters.par')\n",
    "SparkFiles.getRootDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bins: integer (nullable = true)\n",
      " |-- rightAscension: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- declination: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- frequencies: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- pixels: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: float (containsNull = true)\n",
      " |-- keys: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compress3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    fitsFilename='image.restored.i.SB2338.V2.cube.fits'\n",
    "    raType='RA---SIN'\n",
    "    decType='DEC--SIN'\n",
    "    spectraType='Hz'\n",
    "    \n",
    "    raArray, decArray, spectraArray = GetDetailArrays(sqlContext, fitsFilename, raType, decType, spectraType)\n",
    "    \n",
    "    raBucketSize=15\n",
    "    decBucketSize=15\n",
    "    freqBucketSize=15\n",
    "    \n",
    "    raHist=np.histogram(raArray, raBucketSize)\n",
    "    decHist=np.histogram(np.flip(decArray), decBucketSize)\n",
    "    freqHist=np.histogram(spectraArray, freqBucketSize)\n",
    "    arr = np.empty((0,6), float)\n",
    "    \n",
    "    for i in np.arange(raBucketSize):\n",
    "        for j in  np.arange(decBucketSize):\n",
    "            for k in np.arange(freqBucketSize):\n",
    "                x=np.array([[raHist[1][i], raHist[1][i+1], decHist[1][j], decHist[1][j+1], freqHist[1][k], freqHist[1][k+1] ]])\n",
    "                arr=np.append(arr, x, axis=0)\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    hiDec=np.flip(decHist[1])[0]\n",
    "    loDec=np.flip(decHist[1])[1]\n",
    "    hiRa=raHist[1][1]\n",
    "    loRa=raHist[1][0]\n",
    "    loFreq=freqHist[1][0] #spectraArray[0]\n",
    "    hiFreq=freqHist[1][1] #spectraArray[299]\n",
    "    \n",
    "    depthOfCubes=5\n",
    "    \n",
    "    CubeSize=[hiRa,loRa,hiDec,loDec,loFreq,hiFreq]\n",
    "    \n",
    "    frqPix=len( np.array(np.where(np.logical_and(spectraArray >= loFreq, spectraArray <= hiFreq )))[0] )\n",
    "    \n",
    "    ntileCount=frqPix/depthOfCubes\n",
    "    \n",
    "    subCubeDF,raHeaderIndex,decHeaderIndex,freqHeaderIndex,naxis1, naxis2, naxis4 \\\n",
    "    =GetSubCube2(\\\n",
    "                 sqlContext, fitsFilename, decType,spectraType, \\\n",
    "                 raArray, raType, decArray, spectraArray, CubeSize, round(ntileCount) )\n",
    "    \n",
    "    compress3=FlattenDataFrame(sqlContext, subCubeDF, fitsFilename)\n",
    "    \n",
    "    size=GetDataframeSize(sqlContext, compress3)\n",
    "    \n",
    "    print(\"Dataframe size - {} MB\".format(str(size)))\n",
    "    \n",
    "    p=Partitioner()\n",
    "    \n",
    "    rddout=compress3.rdd.mapPartitions(p.processPartition)\n",
    "    print(\"Fred!\")\n",
    "    return rddout\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Caching Raw image data\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 41.5s, CPU 0.2s\n",
      "Fred!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173',\n",
       " '4 173',\n",
       " '5 173',\n",
       " '6 173',\n",
       " '0 173',\n",
       " '1 173',\n",
       " '2 173',\n",
       " '3 173']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.take(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addfiles configuration testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=GetDataframeSize(sqlContext, compress3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1237.75152"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('hdfs:///user/hduser/ConfigurationFiles/SoFiA/default_parameters.par')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/FITs/Spark/tmp/spark-61185fc3-fb12-4761-b548-0d260f02571d/userFiles-5fcb9a1a-e07d-491d-9fcb-305d86fb0e10'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterFile=SparkFiles.get('default_parameters.par')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(parameterFile, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ____________________________________________________________________ ###\n",
      "###                                                                      ###\n",
      "### SoFiA 2.2.1 (default_parameters.par) - Source Finding Application    ###\n",
      "### Copyright (C) 2020 Tobias Westmeier                                  ###\n",
      "### ____________________________________________________________________ ###\n",
      "###                                                                      ###\n",
      "### Address:  Tobias Westmeier                                           ###\n",
      "###           ICRAR M468                                                 ###\n",
      "###           The University of Western Australia                        ###\n",
      "###           35 Stirling Highway                                        ###\n",
      "###           Crawley WA 6009                                            ###\n",
      "###           Australia                                                  ###\n",
      "###                                                                      ###\n",
      "### E-mail:   tobias.westmeier [at] uwa.edu.au                           ###\n",
      "### ____________________________________________________________________ ###\n",
      "###                                                                      ###\n",
      "### This program is free software: you can redistribute it and/or modify ###\n",
      "### it under the terms of the GNU General Public License as published by ###\n",
      "### the Free Software Foundation, either version 3 of the License, or    ###\n",
      "### (at your option) any later version.                                  ###\n",
      "###                                                                      ###\n",
      "### This program is distributed in the hope that it will be useful,      ###\n",
      "### but WITHOUT ANY WARRANTY; without even the implied warranty of       ###\n",
      "### MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the         ###\n",
      "### GNU General Public License for more details.                         ###\n",
      "###                                                                      ###\n",
      "### You should have received a copy of the GNU General Public License    ###\n",
      "### along with this program. If not, see http://www.gnu.org/licenses/.   ###\n",
      "### ____________________________________________________________________ ###\n",
      "###                                                                      ###\n",
      "\n",
      "\n",
      "# List of default parameter settings for SoFiA\n",
      "#\n",
      "# Note that SoFiA's default parameters are actually hard-coded in the\n",
      "# file Parameter.c, and any changes to this file will have no effect.\n",
      "# This file can instead be used as a template for setting up your own\n",
      "# parameter files.\n",
      "\n",
      "\n",
      "# Global settings\n",
      "\n",
      "pipeline.verbose           =  false\n",
      "pipeline.pedantic          =  true\n",
      "pipeline.threads           =  0\n",
      "\n",
      "\n",
      "# Input\n",
      "\n",
      "input.data                 = /mnt1/FITs/image.restored.i.SB2338.V2.cube.fits \n",
      "input.region               = 1200,1800,1350,2005,1,1000 \n",
      "input.gain                 =  \n",
      "input.noise                =  \n",
      "input.weights              =  \n",
      "input.mask                 =  \n",
      "input.invert               =  false\n",
      "\n",
      "\n",
      "# Flagging\n",
      "\n",
      "flag.region                =  \n",
      "flag.catalog               =  \n",
      "flag.radius                =  0\n",
      "flag.auto                  =  false\n",
      "flag.threshold             =  5.0\n",
      "flag.radiusSpatial         =  0\n",
      "flag.log                   =  false\n",
      "\n",
      "\n",
      "# Continuum subtraction\n",
      "\n",
      "contsub.enable             =  false\n",
      "contsub.order              =  0\n",
      "contsub.threshold          =  2.0\n",
      "contsub.shift              =  4\n",
      "contsub.padding            =  3\n",
      "\n",
      "\n",
      "# Noise scaling\n",
      "\n",
      "scaleNoise.enable          =  false\n",
      "scaleNoise.mode            =  spectral\n",
      "scaleNoise.statistic       =  mad\n",
      "scaleNoise.fluxRange       =  negative\n",
      "scaleNoise.windowXY        =  25\n",
      "scaleNoise.windowZ         =  15\n",
      "scaleNoise.gridXY          =  0\n",
      "scaleNoise.gridZ           =  0\n",
      "scaleNoise.interpolate     =  false\n",
      "scaleNoise.scfind          =  false\n",
      "\n",
      "\n",
      "# Spatial filter\n",
      "\n",
      "spatFilter.enable          =  false\n",
      "spatFilter.window          =  50\n",
      "spatFilter.statistic       =  median\n",
      "spatFilter.boxcar          =  0\n",
      "\n",
      "\n",
      "# S+C finder\n",
      "\n",
      "scfind.enable              =  true\n",
      "scfind.kernelsXY           =  0, 3, 6\n",
      "scfind.kernelsZ            =  0, 3, 7, 15\n",
      "scfind.threshold           =  5.0\n",
      "scfind.replacement         =  2.0\n",
      "scfind.statistic           =  mad\n",
      "scfind.fluxRange           =  negative\n",
      "\n",
      "\n",
      "# Threshold finder\n",
      "\n",
      "threshold.enable           =  false\n",
      "threshold.threshold        =  5.0\n",
      "threshold.mode             =  relative\n",
      "threshold.statistic        =  mad\n",
      "threshold.fluxRange        =  negative\n",
      "\n",
      "\n",
      "# Linker\n",
      "\n",
      "linker.radiusXY            =  1\n",
      "linker.radiusZ             =  1\n",
      "linker.minSizeXY           =  5\n",
      "linker.minSizeZ            =  5\n",
      "linker.maxSizeXY           =  0\n",
      "linker.maxSizeZ            =  0\n",
      "linker.keepNegative        =  false\n",
      "\n",
      "\n",
      "# Reliability\n",
      "\n",
      "reliability.enable         =  false\n",
      "reliability.threshold      =  0.9\n",
      "reliability.scaleKernel    =  0.4\n",
      "reliability.fmin           =  15.0\n",
      "reliability.plot           =  true\n",
      "\n",
      "\n",
      "# Mask dilation\n",
      "\n",
      "dilation.enable            =  false\n",
      "dilation.iterationsXY      =  10\n",
      "dilation.iterationsZ       =  5\n",
      "dilation.threshold         =  0.001\n",
      "\n",
      "\n",
      "# Parameterisation\n",
      "\n",
      "parameter.enable           =  true\n",
      "parameter.wcs              =  true\n",
      "parameter.physical         =  false\n",
      "parameter.prefix           =  SoFiA\n",
      "parameter.offset           =  false\n",
      "\n",
      "\n",
      "# Output\n",
      "\n",
      "output.directory           =  /home/hduser/pySparkNotebooks/FITs_Paper/sofia/SoFiA-2 \n",
      "output.filename            =  FranksSwampBabes\n",
      "output.writeCatASCII       =  true\n",
      "output.writeCatXML         =  true\n",
      "output.writeCatSQL         =  true\n",
      "output.writeNoise          =  false\n",
      "output.writeFiltered       =  false\n",
      "output.writeMask           =  false\n",
      "output.writeMask2d         =  false\n",
      "output.writeRawMask        =  false\n",
      "output.writeMoments        =  false\n",
      "output.writeCubelets       =  false\n",
      "output.marginCubelets      =  0\n",
      "output.overwrite           =  true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/FITs/Spark/tmp/spark-61185fc3-fb12-4761-b548-0d260f02571d/userFiles-5fcb9a1a-e07d-491d-9fcb-305d86fb0e10/default_parameters.par'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameterFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('hdfs:///user/hduser/PythonLibraries/myLib.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "zipPath=SparkFiles.get(\"example.zip\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/FITs/Spark/tmp/spark-e31a47db-6cd6-4bfc-a9d1-b952664b0043/userFiles-c6c3a740-b147-451b-a672-5d41f688793e/example.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipPath[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(zipPath[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n"
     ]
    }
   ],
   "source": [
    "import myLib as m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Gordnes stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import threading\n",
    "#import sofia\n",
    "\n",
    "import signal\n",
    "PID = os.getpid()\n",
    "\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "DEBUG = True\n",
    "FITS_HEADER_BLOCK_SIZE = 2880\n",
    "FITS_HEADER_LINE_SIZE  =   80\n",
    "FITS_HEADER_LINES      =   36\n",
    "FITS_HEADER_KEYWORD_SIZE =  8\n",
    "FITS_HEADER_KEY_SIZE     = 10\n",
    "FITS_HEADER_VALUE_SIZE   = 70\n",
    "FITS_HEADER_FIXED_WIDTH  = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr = {}\n",
    "data3D =[[[]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitsfile='/mnt1/FITs/image.restored.i.SB2338.V2.cube.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /mnt1/FITs/image.restored.i.SB2338.V2.cube.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      77   (5607, 5654, 1, 2592)   float32   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not memory map array with mode='readonly', falling back to mode='denywrite', which means that the array will be read-only [astropy.io.fits.file]\n"
     ]
    }
   ],
   "source": [
    "with fits.open(fitsfile) as hdul:\n",
    "    for key in hdul[0].header:\n",
    "        hdr[key] = hdul[0].header[key]\n",
    "        data3D = hdul[0].data\n",
    "        pass\n",
    "    hdul.info()\n",
    "    # delete the END key, if there\n",
    "    hdr.pop(\"END\",None)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SIMPLE': True,\n",
       " 'BITPIX': -32,\n",
       " 'NAXIS': 4,\n",
       " 'NAXIS1': 5607,\n",
       " 'NAXIS2': 5654,\n",
       " 'NAXIS3': 1,\n",
       " 'NAXIS4': 2592,\n",
       " 'BSCALE': 1.0,\n",
       " 'BZERO': 0.0,\n",
       " '': ,\n",
       " 'BUNIT': 'Jy/beam',\n",
       " 'EQUINOX': 2000.0,\n",
       " 'RADESYS': 'FK5',\n",
       " 'LONPOLE': 180.0,\n",
       " 'LATPOLE': -44.83073888889,\n",
       " 'PC01_01': 1.0,\n",
       " 'PC02_01': 0.0,\n",
       " 'PC03_01': 0.0,\n",
       " 'PC04_01': 0.0,\n",
       " 'PC01_02': 0.0,\n",
       " 'PC02_02': 1.0,\n",
       " 'PC03_02': 0.0,\n",
       " 'PC04_02': 0.0,\n",
       " 'PC01_03': 0.0,\n",
       " 'PC02_03': 0.0,\n",
       " 'PC03_03': 1.0,\n",
       " 'PC04_03': 0.0,\n",
       " 'PC01_04': 0.0,\n",
       " 'PC02_04': 0.0,\n",
       " 'PC03_04': 0.0,\n",
       " 'PC04_04': 1.0,\n",
       " 'CTYPE1': 'RA---SIN',\n",
       " 'CRVAL1': 332.6475708333,\n",
       " 'CDELT1': -0.001111111111111,\n",
       " 'CRPIX1': 3175.0,\n",
       " 'CUNIT1': 'deg',\n",
       " 'CTYPE2': 'DEC--SIN',\n",
       " 'CRVAL2': -44.83073888889,\n",
       " 'CDELT2': 0.001111111111111,\n",
       " 'CRPIX2': 3252.0,\n",
       " 'CUNIT2': 'deg',\n",
       " 'CTYPE3': 'STOKES',\n",
       " 'CRVAL3': 1.0,\n",
       " 'CDELT3': 1.0,\n",
       " 'CRPIX3': 1.0,\n",
       " 'CUNIT3': '',\n",
       " 'CTYPE4': 'FREQ',\n",
       " 'CRVAL4': 1376500000.056,\n",
       " 'CDELT4': 18518.51799989,\n",
       " 'CRPIX4': 1.0,\n",
       " 'CUNIT4': 'Hz',\n",
       " 'PV2_1': 0.0,\n",
       " 'PV2_2': 0.0,\n",
       " 'RESTFRQ': 1420405751.786,\n",
       " 'SPECSYS': 'BARYCENT',\n",
       " 'ALTRVAL': 9562377.937478,\n",
       " 'ALTRPIX': 1.0,\n",
       " 'VELREF': 2,\n",
       " 'COMMENT': casacore non-standard usage: 4 LSD, 5 GEO, 6 SOU, 7 GAL,\n",
       " 'DATE': '2019-02-04T15:52:09.925000',\n",
       " 'TIMESYS': 'UTC',\n",
       " 'ORIGIN': 'ASKAPSoft',\n",
       " 'TELESCOP': 'ASKAP',\n",
       " 'PROJECT': 'AS035',\n",
       " 'SBID': '2338',\n",
       " 'DATE-OBS': '2016-10-18T06:21:28.5',\n",
       " 'DURATION': '43207.2',\n",
       " 'HISTORY': Produced with ASKAPsoft version ,\n",
       " Processed with ASKAP pipeline version 0.23.2,\n",
       " Processed with ACES software revision 48354,\n",
       " Processed with ASKAP pipelines on 2019-02-03T17:28:56,\n",
       " Processed with CASA version 5.3.0-143.el7\n",
       " Produced with ASKAPsoft version 0.23.3,\n",
       " Processed with ASKAP pipeline version 0.23.3,\n",
       " Processed with ACES software revision 48354,\n",
       " Processed with ASKAP pipelines on 2019-03-01T07:37:45,\n",
       " Processed with CASA version 5.3.0-143.el7}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2592, 1, 5654, 5607)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82171526976,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3D.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2FITSstr(hdr_dict):\n",
    "    ''' Convert the header dict to a single string matching \n",
    "        the FITS format.\n",
    "    '''\n",
    "    my_str = \"\"\n",
    "    key = \"\"\n",
    "    val = \"\"\n",
    "    hdrsize = 0\n",
    "    for key in hdr_dict.keys():\n",
    "        my_str += key[0:FITS_HEADER_KEYWORD_SIZE] + \" \" * ((FITS_HEADER_KEYWORD_SIZE - len(key)) if len(key) < FITS_HEADER_KEYWORD_SIZE else 0)\n",
    "        my_str += \"= \"\n",
    "        if isinstance(hdr_dict[key],bool):\n",
    "            if hdr_dict[key]:\n",
    "                hdr_dict[key] = \"T\"\n",
    "            else:\n",
    "                hdr_dict[key] = \"F\"\n",
    "        if isinstance(hdr_dict[key],str) and not (key in [\"SIMPLEFITS_HEADER_VALUE_SIZE\",\"EXTEND\",\"COMMENT\",\"HISTORY\"]):\n",
    "                val = \"'%s'\" % hdr_dict[key]\n",
    "        else:\n",
    "            val = \"%s\" % hdr_dict[key]\n",
    "        my_str += val[0:FITS_HEADER_VALUE_SIZE] + \" \" * ((FITS_HEADER_VALUE_SIZE - len(val)) if len(val) < FITS_HEADER_VALUE_SIZE else 0)\n",
    "        hdrsize += FITS_HEADER_LINE_SIZE\n",
    "    if not key == \"END\":\n",
    "        my_str +=  \"END\" + \" \" * (FITS_HEADER_LINE_SIZE - 3)\n",
    "        hdrsize += FITS_HEADER_LINE_SIZE\n",
    "    \n",
    "    pad = (FITS_HEADER_BLOCK_SIZE - hdrsize) if (hdrsize < FITS_HEADER_BLOCK_SIZE) else (FITS_HEADER_BLOCK_SIZE - (hdrsize % FITS_HEADER_BLOCK_SIZE))\n",
    "    for i in range(pad):\n",
    "        my_str += \" \"\n",
    "    hdrsize += pad\n",
    "    return my_str,hdrsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdrstr,hdrsize = dict2FITSstr(hdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5760"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdrsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark SoFiA Testing (Spark 2.3.0, python 3.6, increased resources)",
   "language": "python",
   "name": "sofia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
