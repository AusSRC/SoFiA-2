{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hercules-3.nimbus.pawsey.org.au:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hercules-3.nimbus.pawsey.org.au:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f071e06bb00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n"
     ]
    }
   ],
   "source": [
    "import myLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'example'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-621866f4e6d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'example'"
     ]
    }
   ],
   "source": [
    "import example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----+--------------------+--------------------+\n",
      "|frequency|      rightAscension|bins|         declination|                pixs|\n",
      "+---------+--------------------+----+--------------------+--------------------+\n",
      "|   1.3765|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0, 0.0, 0.0,...|\n",
      "|1.3765185|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0, 0.0, 0.0,...|\n",
      "|1.3765371|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0, 0.0, 0.0,...|\n",
      "|1.3765556|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0, 0.0, 0.0,...|\n",
      "| 1.376574|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0, 0.0, 0.0,...|\n",
      "|1.3765926|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0, 0.0, 0.0,...|\n",
      "|1.3766111|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0056432555, ...|\n",
      "|1.3766296|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-1.20522265E-4...|\n",
      "|1.3766482|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0017296245, ...|\n",
      "|1.3766667|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0033594582, ...|\n",
      "|1.3766851|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.010163352, ...|\n",
      "|1.3767037|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0027894867, ...|\n",
      "|1.3767222|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.013303116, ...|\n",
      "|1.3767407|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.0067061414,...|\n",
      "|1.3767593|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.00709428, -...|\n",
      "|1.3767778|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0077385306, ...|\n",
      "|1.3767962|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-3.9033545E-4,...|\n",
      "|1.3768148|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.0033171072,...|\n",
      "|1.3768333|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.0061068386,...|\n",
      "|1.3768518|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.0051855496, ...|\n",
      "|1.3768704|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.009008285, 0...|\n",
      "|1.3768889|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.0027458335,...|\n",
      "|1.3769073|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.009206427, 0...|\n",
      "| 1.376926|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[3.5444438E-4, ...|\n",
      "|1.3769444|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.001583732, 0...|\n",
      "| 1.376963|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.0068216478,...|\n",
      "|1.3769815|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.00313338, 0....|\n",
      "|    1.377|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.0039892206,...|\n",
      "|1.3770186|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-0.0036204176,...|\n",
      "| 1.377037|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.009736835, 0...|\n",
      "|1.3770555|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.008908393, 0...|\n",
      "|1.3770741|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[-2.7344655E-4,...|\n",
      "|1.3770926|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.003864847, 0...|\n",
      "|1.3771111|[332.39966, 332.3...|   1|[-46.250008, -46....|[[[0.001442909, 0...|\n",
      "|1.3771297|[332.39966, 332.3...|   2|[-46.250008, -46....|[[[-0.0034034138,...|\n",
      "|1.3771482|[332.39966, 332.3...|   2|[-46.250008, -46....|[[[2.0380215E-4, ...|\n",
      "|1.3771666|[332.39966, 332.3...|   2|[-46.250008, -46....|[[[3.304216E-5, -...|\n",
      "|1.3771852|[332.39966, 332.3...|   2|[-46.250008, -46....|[[[-5.108188E-4, ...|\n",
      "|1.3772037|[332.39966, 332.3...|   2|[-46.250008, -46....|[[[-0.011191823, ...|\n",
      "|1.3772222|[332.39966, 332.3...|   2|[-46.250008, -46....|[[[0.020392144, 0...|\n",
      "+---------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"use fits_investigation\")\n",
    "numBins=30\n",
    "sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select 1 as grp, sda_Frequency_hz, spi_index, ra as rightAscension,\n",
    "    map(\n",
    "\n",
    "        'dec', sda_declination\n",
    "    ) as kva,\n",
    "    map(\n",
    "        'pixels', raSelectRange\n",
    "    ) as kvi\n",
    "    from collatedImages\n",
    "    distribute by sda_Frequency_hz\n",
    "    sort by sda_Frequency_hz, spi_index\n",
    ")\n",
    "select \n",
    "    sda_Frequency_hz as frequency,rightAscension,\n",
    "    ntile({}) over (sort by sda_Frequency_hz) as bins,\n",
    "    collect_list(float(a.kva['dec']))as declination\n",
    "    ,collect_list(array(a.kvi['pixels']))as pixs\n",
    "from rawData a\n",
    "group by sda_Frequency_hz, rightAscension\n",
    "\"\"\".format(numBins) ).show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark2 command line testing\n",
    "\n",
    "### The basic dataframe creation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark2 \\\n",
    "--py-files 'hdfs:///user/hduser/PythonLibraries/myLib.py' \\\n",
    "--archives 'hdfs:///user/hduser/SharedObjectDLLs/example.zip#example' \\\n",
    "--conf spark.network.timeout=50000 \\\n",
    "--conf spark.executor.heartbeatInterval=240 \\\n",
    "--conf spark.shuffle.service.enabled=True \\\n",
    "--conf spark.sql.shuffle.partitions=30 \\\n",
    "--conf spark.default.parallelism=30 \\\n",
    "--conf spark.dynamicAllocation.enabled=True \\\n",
    "--conf spark.dynamicAllocation.executorIdleTimeout=600 \\\n",
    "--conf spark.network.timeout=800 \\\n",
    "--conf spark.executor.instances=30 \\\n",
    "--conf \"spark.executor.extraLibraryPath=:./example\" \\\n",
    "--conf \"spark.driver.extraLibraryPath=:./example\" \\\n",
    "--conf spark.executor.memoryOverhead='4g' \\\n",
    "--conf spark.driver.memoryOverhead='4g' \\\n",
    "--conf spark.rpc.message.maxSize='512' \\\n",
    "--conf spark.scheduler.mode='FAIR' \\\n",
    "--conf spark.kryoserializer.buffer.max='1g' \n",
    "\n",
    "spark.sql.codegen.wholeStage=False - to avoid GeneratedIteratorForCOdegenStage grows beyond 64 KB errors \n",
    "\n",
    "pyspark2 \\\n",
    "--py-files 'hdfs:///user/hduser/PythonLibraries/myLib.py' \\\n",
    "--archives 'hdfs:///user/hduser/SharedObjectDLLs/example.zip#example' \\\n",
    "--conf spark.network.timeout=50000 \\\n",
    "--conf spark.executor.heartbeatInterval=240 \\\n",
    "--conf spark.shuffle.service.enabled=True \\\n",
    "--conf spark.dynamicAllocation.enabled=True \\\n",
    "--conf spark.maxRemoteBlockSizeFetchToMem='2g' \\\n",
    "--conf spark.dynamicAllocation.executorIdleTimeout=600 \\\n",
    "--conf spark.network.timeout=800 \\\n",
    "--conf spark.sql.codegen.wholeStage=False \\\n",
    "--conf \"spark.executor.extraLibraryPath=:./example\" \\\n",
    "--conf \"spark.driver.extraLibraryPath=:./example\" \\\n",
    "--conf spark.executor.memoryOverhead='4g' \\\n",
    "--conf spark.driver.memoryOverhead='4g' \\\n",
    "--conf spark.driver.maxResultSize='4g' \\ \n",
    "--conf spark.rpc.message.maxSize='512' \\\n",
    "--conf spark.scheduler.mode='FAIR' \\\n",
    "--conf spark.kryoserializer.buffer.max='1g' \n",
    "\n",
    "sc.stop()\n",
    "spark.stop()\n",
    "SparkSession._instantiatedContext = None\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "spark=SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "    \n",
    "import example\n",
    "import myLib\n",
    "\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random as rnd\n",
    "from time import sleep\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark import StorageLevel\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import randn, monotonically_increasing_id, row_number, desc, udf, col, lit\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType,Row, BooleanType, StringType\n",
    "from pyspark.sql.types import MapType, StructType,StructField\n",
    "from myLib import GetDetailArrays,GetSubCube2,GetDataframeSize,CreateFITSSubCubeUDF ,FlattenDataFrame\n",
    "\"\"\"\n",
    "Set the Spark datatypes\n",
    "\"\"\"\n",
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector,VectorUDT\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time as tm\n",
    "import logging\n",
    "from time import time, clock\n",
    "\n",
    "sqlContext.sql(\"use fits_investigation\")\n",
    "fitsFilename='image.restored.i.SB2338.V2.cube.fits'\n",
    "raType='RA---SIN'\n",
    "decType='DEC--SIN'\n",
    "spectraType='GHz'\n",
    "\n",
    "numBins=30\n",
    "raArray, decArray, spectraArray = GetDetailArrays(sqlContext,fitsFilename, raType, decType, spectraType)\n",
    "\n",
    "hiDec=-46.25\n",
    "loDec=-47.0\n",
    "hiRa=332.4\n",
    "loRa=331.4\n",
    "loFreq=spectraArray[0]\n",
    "hiFreq=spectraArray[1000]\n",
    "CubeSize=[hiRa,loRa,hiDec,loDec,loFreq,hiFreq]\n",
    "raPix=len( np.array(np.where(np.logical_and(raArray >= loRa, raArray <= hiRa )))[0] )\n",
    "decPix=len( np.array(np.where(np.logical_and(decArray >= loDec, decArray <= hiDec )))[0] )\n",
    "frqPix=len( np.array(np.where(np.logical_and(spectraArray >= loFreq, spectraArray <= hiFreq )))[0] )\n",
    "print(raPix, decPix, frqPix)\n",
    "\n",
    "size=raPix*decPix*frqPix*32/8e9 # 8e6 = MB, 8e9= GB\n",
    "print(size)\n",
    "\n",
    "cacheTempTables=False\n",
    "\n",
    "subCubeDF,raHeaderIndex,decHeaderIndex,freqHeaderIndex,naxis1, naxis2, naxis4 \\\n",
    "        =GetSubCube2(sqlContext, fitsFilename, decType,spectraType, raArray, \\\n",
    "                     raType, decArray, spectraArray, CubeSize, cacheTempTables)\n",
    "subCubeDF.createOrReplaceTempView(\"collatedImages\")\n",
    "\n",
    "compress1=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select 1 as grp, sda_Frequency_hz, spi_index, ra as rightAscension,\n",
    "    map(\n",
    "\n",
    "        'dec', sda_declination\n",
    "    ) as kva,\n",
    "    map(\n",
    "        'pixels', raSelectRange\n",
    "    ) as kvi\n",
    "    from collatedImages\n",
    "    distribute by sda_Frequency_hz\n",
    "    sort by sda_Frequency_hz, spi_index\n",
    ")\n",
    "select \n",
    "    sda_Frequency_hz as frequency,rightAscension,\n",
    "    --ntile({}) over (sort by sda_Frequency_hz) as bins,\n",
    "    collect_list(float(a.kva['dec']))as declination\n",
    "    ,collect_list(array(a.kvi['pixels']))as pixs\n",
    "from rawData a\n",
    "group by frequency, rightAscension\n",
    "\"\"\".format(numBins))#.cache()\n",
    "\n",
    "compress1.createOrReplaceTempView(\"compress_one\")\n",
    "\n",
    "compress2=sqlContext.sql(\"\"\"\n",
    "select\n",
    "    bins,\n",
    "    rightAscension, declination,\n",
    "    collect_list(array(float(a.kvi['frequencies']))) as frequencies,\n",
    "    collect_list(array(a.kva['pixs'])) as pixels\n",
    "from (\n",
    "    select\n",
    "        1 as bins,\n",
    "        rightAscension, declination,\n",
    "        map('frequencies', frequency) as kvi,\n",
    "        map('pixs', pixs) as kva\n",
    "    from compress_one\n",
    "    distribute by rightAscension\n",
    "    sort by frequency\n",
    ") a\n",
    "group by bins,\n",
    "rightAscension, declination\n",
    "\"\"\")\n",
    "\n",
    "sqlContext.sql(\"\"\"\n",
    "    select 1 as grp, sfh_index, sfh_key, sfh_value \n",
    "    from sparkfits_fits_headers\n",
    "    where sfh_fits_file='{}' \n",
    "    order by sfh_index\n",
    "\"\"\".format(fitsFilename)\n",
    ").createOrReplaceTempView(\"headers\")\n",
    "\n",
    "originalHeader=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select grp, sfh_index, \n",
    "    map(\n",
    "\n",
    "        'key', sfh_key\n",
    "    ) as kva,\n",
    "    map(\n",
    "        'value', sfh_value\n",
    "    ) as kvi\n",
    "    from ( --headers\n",
    "        select 1 as grp, sfh_index, sfh_key, sfh_value \n",
    "        from sparkfits_fits_headers\n",
    "        where sfh_fits_file='{}' \n",
    "        order by sfh_index    \n",
    "    ) a\n",
    "    distribute by grp\n",
    "    sort by grp, sfh_index\n",
    ")\n",
    "select \n",
    "    grp,\n",
    "    collect_list(string(a.kva['key']))as keys\n",
    "    ,collect_list(string(a.kvi['value']))as values\n",
    "from rawData a\n",
    "group by grp\n",
    "\"\"\".format(fitsFilename)).select(\"keys\",\"values\")\n",
    "\n",
    "compress3 = compress2.crossJoin(originalHeader)\n",
    "\n",
    "compress3 = compress3.repartition(\"bins\")\n",
    "\n",
    "test1 = udf(CreateFITSSubCubeUDF, StringType() ) \n",
    "\n",
    "dfout=compress3.filter(compress3.bins <= 10)\\\n",
    "    .withColumn('process_msg', test1(compress3.rightAscension, \\\n",
    "                                           compress3.declination, \\\n",
    "                                           compress3.frequencies, \\\n",
    "                                           compress3.pixels, \\\n",
    "                                           compress3.keys, \\\n",
    "                                           compress3.values, \\\n",
    "                                           f.lit(fitsFilename))).select('bins','process_msg').persist()   \n",
    "\n",
    "dfout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2-submit --master yarn --deploy-mode client --py-files 'hdfs:///user/hduser/PythonLibraries/myLib.py' \\\n",
    "--archives 'hdfs:///user/hduser/SharedObjectDLLs/example.zip#example' \\\n",
    "--conf \"spark.executor.extraLibraryPath=:./example\" \\\n",
    "--conf \"spark.driver.extraLibraryPath=:./example\" \\\n",
    "DistributedSoFiA_Framework.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2-submit  \\\n",
    "  --master yarn \\\n",
    "  --name 'Distributed SoFiA' \\\n",
    "  --deploy-mode client \\\n",
    "  --driver-memory 10g \\\n",
    "  --executor-memory 10g \\\n",
    "  --executor-cores 5 \\\n",
    "  --num-executors 23 \\\n",
    "  --py-files 'hdfs:///user/hduser/PythonLibraries/myLib.py' \\\n",
    "  --archives 'hdfs:///user/hduser/SharedObjectDLLs/example.zip#example' \\\n",
    "  --conf \"spark.executor.extraLibraryPath=:./example\" \\\n",
    "  --conf \"spark.driver.extraLibraryPath=:./example\" \\\n",
    "  --conf spark.pyspark.python=/home/hduser/.virtualenvs/Elephas/bin/python \\\n",
    "  --conf spark.pyspark.driver.python=/home/hduser/.virtualenvs/Elephas/bin/python \\\n",
    "  --conf spark.shuffle.service.enabled=True \\\n",
    "  --conf spark.sql.shuffle.partitions=23 \\\n",
    "  --conf spark.default.parallelism=23 \\\n",
    "  --conf spark.sql.codegen.wholeStage=False \\\n",
    "  --conf spark.dynamicAllocation.enabled=True \\\n",
    "  --conf spark.executor.memoryOverhead='1g' \\\n",
    "  --conf spark.driver.memoryOverhead='4g' \\\n",
    "  --conf spark.rpc.message.maxSize='512' \\\n",
    "  --conf spark.scheduler.mode='FAIR' \\\n",
    "  --conf spark.kryoserializer.buffer.max='1g' \\\n",
    "  --conf spark.yarn.appMasterEnv.ELEPHAS='True' \\\n",
    "  --conf spark.port.maxRetries=32 \\\n",
    "  --conf spark.driver.maxResultSize='4096m' \\\n",
    " DistributedSoFiA_Framework.py\n",
    "\n",
    "https://community.cloudera.com/t5/Community-Articles/Spark-job-fails-with-below-error-when-byte-code-grows-beyond/ta-p/248494\n",
    "    The codegen wholeStage FALSE directive\n",
    "\n",
    "spark2-submit  \\\n",
    "  --master yarn \\\n",
    "  --name 'Distributed SoFiA' \\\n",
    "  --deploy-mode client \\\n",
    "  --executor-memory 8g \\\n",
    "  --driver-memory 8g \\\n",
    "  --executor-cores 4 \\\n",
    "  --num-executors 29 \\\n",
    "  --py-files 'hdfs:///user/hduser/PythonLibraries/myLib.py' \\\n",
    "  --archives 'hdfs:///user/hduser/SharedObjectDLLs/example.zip#example' \\\n",
    "  --conf \"spark.executor.extraLibraryPath=:./example\" \\\n",
    "  --conf \"spark.driver.extraLibraryPath=:./example\" \\\n",
    "  --conf spark.pyspark.python=/home/hduser/.virtualenvs/Elephas/bin/python \\\n",
    "  --conf spark.pyspark.driver.python=/home/hduser/.virtualenvs/Elephas/bin/python \\\n",
    "  --conf spark.sql.shuffle.partitions=480 \\\n",
    "  --conf spark.default.parallelism=480 \\\n",
    "  --conf spark.shuffle.service.enabled=True \\\n",
    "  --conf spark.sql.codegen.wholeStage=False \\\n",
    "  --conf spark.dynamicAllocation.enabled=True \\\n",
    "  --conf spark.dynamicAllocation.executorIdleTimeout=800 \\\n",
    "  --conf spark.executor.memoryOverhead='4g' \\\n",
    "  --conf spark.driver.memoryOverhead='600m' \\\n",
    "  --conf spark.rpc.message.maxSize='512' \\\n",
    "  --conf spark.scheduler.mode='FAIR' \\\n",
    "  --conf spark.kryoserializer.buffer.max='1g' \\\n",
    "  --conf spark.yarn.appMasterEnv.ELEPHAS='True' \\\n",
    "  --conf spark.port.maxRetries=32 \\\n",
    "  --conf spark.driver.maxResultSize='4096m' \\\n",
    " DistributedSoFiA_Framework.py > sofia.log 2> sofia_err.log\n",
    "\n",
    "spark2-submit  \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --py-files 'hdfs:///user/hduser/PythonLibraries/myLib.py' \\\n",
    "  --archives 'hdfs:///user/hduser/SharedObjectDLLs/example.zip#example' \\\n",
    "  --conf 'spark.executor.extraLibraryPath=:./example' \\\n",
    "  --conf 'spark.driver.extraLibraryPath=:./example' \\\n",
    "fred.py > fred.log 2> fred_err.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(1185/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatemod7(val):\n",
    "    sys.path.append('example')\n",
    "    import example\n",
    "    return example.my_mod(val, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use fits_investigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|partition                                                      |\n",
      "+---------------------------------------------------------------+\n",
      "|spi_filename=__HIVE_DEFAULT_PARTITION__/spi_band=1             |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=0   |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1   |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=10  |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=100 |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1000|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1001|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1002|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1003|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1004|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1005|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1006|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1007|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1008|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1009|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=101 |\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1010|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1011|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1012|\n",
      "|spi_filename=image.restored.i.SB2338.V2.cube.fits/spi_band=1013|\n",
      "+---------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show partitions sparkfits_images\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import example\n",
    "import myLib\n",
    "\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random as rnd\n",
    "from time import sleep\n",
    "from pyspark.sql import SQLContext, SparkSession, HiveContext\n",
    "from pyspark import StorageLevel\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import randn, monotonically_increasing_id, row_number, desc, udf, col, lit\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType,Row, BooleanType, StringType\n",
    "from pyspark.sql.types import MapType, StructType,StructField\n",
    "from myLib import GetDetailArrays,GetSubCube2,GetDataframeSize,CreateFITSSubCubeUDF, FlattenDataFrame \n",
    "\"\"\"\n",
    "Set the Spark datatypes\n",
    "\"\"\"\n",
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector,VectorUDT\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time as tm\n",
    "import logging\n",
    "from time import time, clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|spi_index|\n",
      "+---------+\n",
      "|        0|\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "|       10|\n",
      "|       11|\n",
      "|       12|\n",
      "|       13|\n",
      "|       14|\n",
      "|       15|\n",
      "|       16|\n",
      "|       17|\n",
      "|       18|\n",
      "|       19|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    select spi_index from sparkfits_images\n",
    "    where spi_filename='image.restored.i.SB2338.V2.cube.fits'\n",
    "    and spi_band=24    \n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partitioner:\n",
    "    def __init__(self):\n",
    "        self.callPerDriverSetup\n",
    "        \n",
    "    def callPerDriverSetup(self):\n",
    "        pass\n",
    "    \n",
    "    def callPerPartitionSetup(self):\n",
    "        sys.path.append('example')                  ### <=== Either append or use the --pyfiles parameter\n",
    "        import example\n",
    "         \n",
    "        self.example=example\n",
    "        \n",
    "    def doProcess(self, element):\n",
    "        ###  here's the call to the C library for each row of the dataframe partition\n",
    "        ### In here we have to transform the flattened array data to the format SoFiA\n",
    "        ### requires, as well as the\n",
    "        return self.example.my_mod(element.spi_index, 7)\n",
    "        ## return self.example.my_mod(element.wire, 7) \n",
    "    \n",
    "    def processPartition(self, partition):\n",
    "        self.callPerPartitionSetup()\n",
    "        for element in partition:\n",
    "            yield self.doProcess(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df=sqlContext.sql(\"\"\"\n",
    "    select spi_index,spi_filename from sparkfits_images\n",
    "    where spi_filename='image.restored.i.SB2338.V2.cube.fits'\n",
    "    and spi_band=24    \n",
    "    \"\"\")\n",
    "    calcmod7 = udf(calculatemod7, IntegerType())\n",
    "    dfout=df.withColumn('calc_mod7', calcmod7(df.spi_index)).select('calc_mod7')\n",
    "    dfout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df=sqlContext.sql(\"\"\"\n",
    "    select spi_index from sparkfits_images\n",
    "    where spi_filename='image.restored.i.SB2338.V2.cube.fits'\n",
    "    and spi_band=24    \n",
    "    \"\"\")\n",
    "    p=Partitioner()\n",
    "    rddout=df.rdd.mapPartitions(p.processPartition)\n",
    "    print(\"Done!\")\n",
    "    rddout.take(20)\n",
    "    print(\"Fred!\")\n",
    "    return rddout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    fitsFilename='image.restored.i.SB2338.V2.cube.fits'\n",
    "    raType='RA---SIN'\n",
    "    decType='DEC--SIN'\n",
    "    spectraType='Hz'\n",
    "    \n",
    "    raArray, decArray, spectraArray = GetDetailArrays(sqlContext, fitsFilename, raType, decType, spectraType)\n",
    "    \n",
    "    raBucketSize=15\n",
    "    decBucketSize=15\n",
    "    freqBucketSize=15\n",
    "    \n",
    "    raHist=np.histogram(raArray, raBucketSize)\n",
    "    decHist=np.histogram(np.flip(decArray), decBucketSize)\n",
    "    freqHist=np.histogram(spectraArray, freqBucketSize)\n",
    "    arr = np.empty((0,6), float)\n",
    "    \n",
    "    for i in np.arange(raBucketSize):\n",
    "        for j in  np.arange(decBucketSize):\n",
    "            for k in np.arange(freqBucketSize):\n",
    "                x=np.array([[raHist[1][i], raHist[1][i+1], decHist[1][j], decHist[1][j+1], freqHist[1][k], freqHist[1][k+1] ]])\n",
    "                arr=np.append(arr, x, axis=0)\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    hiDec=np.flip(decHist[1])[0]\n",
    "    loDec=np.flip(decHist[1])[1]\n",
    "    hiRa=raHist[1][1]\n",
    "    loRa=raHist[1][0]\n",
    "    loFreq=freqHist[1][0] #spectraArray[0]\n",
    "    hiFreq=freqHist[1][1] #spectraArray[299]\n",
    "    \n",
    "    depthOfCubes=5\n",
    "    \n",
    "    CubeSize=[hiRa,loRa,hiDec,loDec,loFreq,hiFreq]\n",
    "    \n",
    "    frqPix=len( np.array(np.where(np.logical_and(spectraArray >= loFreq, spectraArray <= hiFreq )))[0] )\n",
    "    \n",
    "    ntileCount=frqPix/depthOfCubes\n",
    "    \n",
    "    subCubeDF,raHeaderIndex,decHeaderIndex,freqHeaderIndex,naxis1, naxis2, naxis4 \\\n",
    "    =GetSubCube2(\\\n",
    "                 sqlContext, fitsFilename, decType,spectraType, \\\n",
    "                 raArray, raType, decArray, spectraArray, CubeSize, round(ntileCount) )\n",
    "    \n",
    "    compress3=FlattenDataFrame(sqlContext, subCubeDF, fitsFilename)\n",
    "    \n",
    "    p=Partitioner()\n",
    "    \n",
    "    rddout=compress3.rdd.mapPartitions(p.processPartition)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Fred!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.take(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f033470fb00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('hdfs:///user/hduser/PythonLibraries/myLib.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('hdfs:///user/hduser/SharedObjectDLLs/example.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "zipPath=SparkFiles.get(\"example.zip\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/FITs/Spark/tmp/spark-e31a47db-6cd6-4bfc-a9d1-b952664b0043/userFiles-c6c3a740-b147-451b-a672-5d41f688793e/example.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipPath[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(zipPath[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/config\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n",
      "maindir is /home/hduser/.astropy/cache\n"
     ]
    }
   ],
   "source": [
    "import myLib as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_example'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/mnt/FITs/Spark/tmp/spark-e31a47db-6cd6-4bfc-a9d1-b952664b0043/userFiles-c6c3a740-b147-451b-a672-5d41f688793e/example.zip/example.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_example'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Elephas/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named '_example'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-621866f4e6d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/FITs/Spark/tmp/spark-e31a47db-6cd6-4bfc-a9d1-b952664b0043/userFiles-c6c3a740-b147-451b-a672-5d41f688793e/example.zip/example.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/FITs/Spark/tmp/spark-e31a47db-6cd6-4bfc-a9d1-b952664b0043/userFiles-c6c3a740-b147-451b-a672-5d41f688793e/example.zip/example.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_example'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0m_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_example'"
     ]
    }
   ],
   "source": [
    "import example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"use fits_investigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sqlContext.sql(\"select * from sparkfits_images where spi_filename='ASKAPsim_pol1.image.I.restored.fits' and spi_band=12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=m.GetDataframeSize(sc, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark SoFiA Testing (Spark 2.3.0, python 3.6, increased resources)",
   "language": "python",
   "name": "sofia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
