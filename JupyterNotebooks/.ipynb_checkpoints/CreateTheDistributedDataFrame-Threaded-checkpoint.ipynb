{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten a dataframe into a single row\n",
    "\n",
    "https://stackoverflow.com/questions/50556537/flatten-dataframe-into-a-single-row - actually, no - this is the Pandas solution.\n",
    "\n",
    "Try this instead -\n",
    "\n",
    "https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Spark Context\n",
    "\n",
    "We reset the spark context to test and optimise the parameters we will need to include when batch submitting in cluster mode.\n",
    "\n",
    "                           ('spark.maxRemoteBlockSizeFetchToMem', 2048), \\\n",
    "\n",
    "\n",
    "#### Optimising the partitions\n",
    "\n",
    "https://luminousmen.com/post/spark-tips-partition-tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBins=30 ## code in here to optimise and calculate the number of bins we want!\n",
    "\n",
    "\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '10g'),\\\n",
    "                           ('spark.driver.memory', '10g'),\\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.sql.shuffle.partitions', 768), \\\n",
    "                           ('spark.maxRemoteBlockSizeFetchToMem', '2g'), \\\n",
    "                           ('spark.default.parallelism', 768),\\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.network.timeout', 1200), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           #('spark.executor.instances', numBins), \\\n",
    "                           ('spark.sql.codegen.wholeStage', False ), \\\n",
    "                           # to avoid GeneratedIteratorForCOdegenStage grows beyond 64 KB errors\n",
    "                           # overridden('spark.local.dir','/mnt/FITs/Spark/tmp'),\\\n",
    "                           ('spark.jars.packages', 'com.github.astrolabsoftware:spark-fits_2.11:0.9.0'),\\\n",
    "                           ('spark.executor.memoryOverhead', 4096),\\\n",
    "                           # deprecated ('spark.yarn.executor.memoryOverhead', 4096), \\\n",
    "                           ('spark.driver.memoryOverhead', '600m'),\\\n",
    "                           ('spark.driver.maxResultSize', '4g'),\\\n",
    "                           ('spark.rpc.message.maxSize', '512'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '1g'),\\\n",
    "                           ('spark.driver.allowMultipleContexts', True), \\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition', True), \\\n",
    "                           ('spark.hadoop.hive.exec.dynamic.partition.mode','nonstrict'), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions', 100000), \\\n",
    "                           ('spark.hadoop.hive.exec.max.dynamic.partitions.pernode', 100000), \\\n",
    "                           ('spark.app.name','Distributed SoFiA Test - dataframe tuning')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    sc.stop()\n",
    "    spark.stop()\n",
    "    SparkSession._instantiatedContext = None\n",
    "\n",
    "    sc=SparkContext(conf=conf)\n",
    "    spark=SparkSession(sc)\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hercules-3.nimbus.pawsey.org.au:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Distributed SoFiA Test - dataframe tuning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=Distributed SoFiA Test - dataframe tuning>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7fa674ae6208>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import py4j.protocol  \n",
    "from py4j.protocol import Py4JJavaError  \n",
    "from py4j.java_gateway import JavaObject  \n",
    "from py4j.java_collections import JavaArray, JavaList\n",
    "\n",
    "from pyspark import RDD, SparkContext  \n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler,VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.functions import udf, col, rand\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType,IntegerType, DataType, DoubleType, MapType, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from myLib import GetDetailArrays, GetSubCube2, GetDataframeSize, CreateFITSSubCubeUDF, writeResults \n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()\n",
    "\n",
    "aType=ArrayType(fType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, clock\n",
    "from datetime import datetime\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    a simple class for printing time (s) since last call\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        self.elapsed = 1\n",
    "        self.elapsedCPU = 1\n",
    "    \n",
    "    def start(self):\n",
    "        self.t0=time()\n",
    "        self.t1=clock()\n",
    "        \n",
    "    def stop(self):\n",
    "        t1=time()\n",
    "        t2=clock()\n",
    "        print(\"Elapsed {:2.1f}s, CPU {:2.1f}s\".format(t1-self.t0, t2-self.t1))\n",
    "        self.elapsed = t1-self.t0\n",
    "        self.elapsedCPU = t2-self.t1\n",
    "\n",
    "timer=Timer()\n",
    "\n",
    "class WriteDataframeError(Exception): \n",
    "  \n",
    "    # Constructor or Initializer \n",
    "    def __init__(self, value): \n",
    "        self.value = value \n",
    "  \n",
    "    # __str__ is to print() the value \n",
    "    def __str__(self): \n",
    "        return(repr(self.value)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\n",
    "    \"name\": s.name(),     \n",
    "    \"memSize_MB\": float(s.memSize())/ 2**20 , \n",
    "    \"memSize_GB\": float(s.memSize())/ 2**30, \n",
    "    \"diskSize_MB\": float(s.diskSize())/ 2**20, \n",
    "    \"diskSize_GB\": float(s.diskSize())/ 2**30, \n",
    "    \"numPartitions\": s.numPartitions(), \n",
    "    \"numCachedPartitions\": s.numCachedPartitions(),\n",
    "    \"callSite\": s.callSite(),\n",
    "    \"externalBlockStoreSize\": s.externalBlockStoreSize(),\n",
    "    \"id\": s.id(),\n",
    "    \"isCached\": s.isCached(),\n",
    "    \"parentIds\": s.parentIds(),\n",
    "    \"scope\": s.scope(),\n",
    "    \"storageLevel\": s.storageLevel(),\n",
    "    \"toString\": s.toString()\n",
    "} for s in sc._jsc.sc().getRDDStorageInfo()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sc._jsc.sc().getRDDStorageInfo():\n",
    "    print(s.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug testing \n",
    "\n",
    "Compares the process times when we cache the temp pables against when we don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheTempTables=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_elapsed=time()\n",
    "start_cpu=clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql('use fits_investigation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------+\n",
      "|            col_name|   data_type|comment|\n",
      "+--------------------+------------+-------+\n",
      "|           spi_index|         int|   null|\n",
      "|           spi_image|array<float>|   null|\n",
      "|        spi_filename|      string|   null|\n",
      "|            spi_band|         int|   null|\n",
      "|# Partition Infor...|            |       |\n",
      "|          # col_name|   data_type|comment|\n",
      "|        spi_filename|      string|   null|\n",
      "|            spi_band|         int|   null|\n",
      "+--------------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('desc sparkfits_images').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ERROR! 'Table or view not found: collatedImages;'\n",
      " ERROR! 'Table or view not found: compress_one;'\n",
      " ERROR! 'Table or view not found: compress_two;'\n",
      " ERROR! 'Table or view not found: compress_three;'\n"
     ]
    }
   ],
   "source": [
    "for tName in (\"collatedImages\", \"compress_one\", \"compress_two\", \"compress_three\"):\n",
    "    try:\n",
    "        sqlContext.sql(\"drop table {}\".format(tName))\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        msg = \" ERROR! \"\n",
    "        if hasattr(e, 'message'):\n",
    "            msg += str(e.message)\n",
    "        else:\n",
    "\n",
    "            msg += str(e)\n",
    "            pass\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitsFilename='residual.i.SB8170.cube.fits.NaNs.Removed'\n",
    "fitsFilename='image.restored.i.SB2338.V2.cube.fits'\n",
    "raType='RA---SIN'\n",
    "decType='DEC--SIN'\n",
    "spectraType='Hz'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "raArray, decArray, spectraArray = GetDetailArrays(sqlContext, fitsFilename, raType, decType, spectraType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of Ra 337.9596862792969 to 328.57586669921875\n",
      "Range of Declination -42.05752944946289 to -48.32979202270508\n",
      "Frequency ranges in Hz - 1376499968.0 1424481536.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Range of Ra {} to {}\".format( max(raArray), min(raArray)))\n",
    "print(\"Range of Declination {} to {}\".format( max(decArray), min(decArray)))\n",
    "print(\"Frequency ranges in {} - {} {}\".format(spectraType, min(spectraArray), max(spectraArray))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from multiprocessing.pool import ThreadPool\n",
    "#import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessThread(inputArray):\n",
    "    # Random sleep to stagger the processes\n",
    "    if random.randint(0,1):\n",
    "        sleep(randint(100,400)/1000)\n",
    "        pass\n",
    "    #count=sqlContext.sql(\"select count(*)as count from sparkfits_detail_arrays\").select(\"count\")\n",
    "    #print(\"Hi there. loRa {} - HiRA {}\".format(str(funkyArray[0]), str(funkyArray[1]) ))\n",
    "    #print(\"Hi there. loDec {} - HiDec {}\".format(str(funkyArray[2]), str(funkyArray[3]) ))\n",
    "    #print(\"Hi there. loFreq {} - HiFreq {}\".format(str(funkyArray[4]), str(funkyArray[5]) ))\n",
    "    \n",
    "    \n",
    "    hiDec=inputArray[3]\n",
    "    loDec=inputArray[2]\n",
    "    hiRa=inputArray[1]\n",
    "    loRa=inputArray[0]\n",
    "    loFreq=inputArray[4]\n",
    "    hiFreq=inputArray[5]\n",
    "    \n",
    "    print(\"hiDec {} loDec {} hiRa {} loRa {} loFreq {} hiFreq {}\".format( str(hiDec), str(loDec),str(hiRa),str(loRa),str(loFreq),str(hiFreq) ))\n",
    "\n",
    "    depthOfCubes=5\n",
    "\n",
    "    CubeSize=[hiRa,loRa,hiDec,loDec,loFreq,hiFreq]\n",
    "    print(\"Cube size is \",CubeSize)\n",
    "    \n",
    "    raPix=len( np.array(np.where(np.logical_and(raArray >= loRa, raArray <= hiRa )))[0] )\n",
    "    decPix=len( np.array(np.where(np.logical_and(decArray >= loDec, decArray <= hiDec )))[0] )\n",
    "    frqPix=len( np.array(np.where(np.logical_and(spectraArray >= loFreq, spectraArray <= hiFreq )))[0] )\n",
    "    print(raPix, decPix, frqPix)\n",
    "\n",
    "    size=raPix*decPix*frqPix*32/8e9 # 8e6 = MB, 8e9= GB\n",
    "    print(size)\n",
    "\n",
    "    ntileCount=frqPix/depthOfCubes\n",
    "    \n",
    "    subCubeDF,raHeaderIndex,decHeaderIndex,freqHeaderIndex,naxis1, naxis2, naxis4 \\\n",
    "    =GetSubCube2(sqlContext, fitsFilename, decType,spectraType, raArray, raType, \\\n",
    "                 decArray, spectraArray, CubeSize, round(ntileCount) )\n",
    "        \n",
    "    running=True\n",
    "    i=0\n",
    "    runTries=10\n",
    "    while running and i <= runTries:\n",
    "        try:\n",
    "            if i !=0:\n",
    "                # second try, random sleep\n",
    "                sleep(randint(100,400)/1000)\n",
    "                \n",
    "            print(\"GetSubCube2 created - creating temp table\")\n",
    "            subCubeDF.createOrReplaceTempView(\"collatedImages\")\n",
    "            \n",
    "            compress1=sqlContext.sql(\"\"\"\n",
    "                with rawData as\n",
    "                (\n",
    "                    select bins, sda_Frequency_hz, spi_index, ra as rightAscension,\n",
    "                    map(\n",
    "\n",
    "                        'dec', sda_declination\n",
    "                    ) as kva,\n",
    "                    map(\n",
    "                        'pixels', raSelectRange\n",
    "                    ) as kvi\n",
    "                    from collatedImages\n",
    "                    distribute by sda_Frequency_hz\n",
    "                    sort by sda_Frequency_hz, spi_index\n",
    "                )\n",
    "                select \n",
    "                    sda_Frequency_hz as frequency,rightAscension,\n",
    "                    bins,\n",
    "                    collect_list(float(a.kva['dec']))as declination\n",
    "                    ,collect_list(array(a.kvi['pixels']))as pixs\n",
    "                from rawData a\n",
    "                group by frequency, rightAscension, bins            \n",
    "            \"\"\")\n",
    "            compress1.createOrReplaceTempView(\"compress_one\")\n",
    "            \n",
    "            compress2=sqlContext.sql(\"\"\"\n",
    "                select\n",
    "                    bins,\n",
    "                    rightAscension, declination,\n",
    "                    collect_list(array(float(a.kvi['frequencies']))) as frequencies,\n",
    "                    collect_list(array(a.kva['pixs'])) as pixels\n",
    "                from (\n",
    "                    select\n",
    "                        bins,\n",
    "                        rightAscension, declination,\n",
    "                        map('frequencies', frequency) as kvi,\n",
    "                        map('pixs', pixs) as kva\n",
    "                    from compress_one\n",
    "                    distribute by rightAscension\n",
    "                    sort by frequency\n",
    "                ) a\n",
    "                group by bins,\n",
    "                rightAscension, declination            \n",
    "            \"\"\")\n",
    "            \n",
    "            originalHeader=sqlContext.sql(\"\"\"\n",
    "                with rawData as\n",
    "                (\n",
    "                    select grp, sfh_index, \n",
    "                    map(\n",
    "\n",
    "                        'key', sfh_key\n",
    "                    ) as kva,\n",
    "                    map(\n",
    "                        'value', sfh_value\n",
    "                    ) as kvi\n",
    "                    from (\n",
    "                    --headers\n",
    "                        select 1 as grp, sfh_index, sfh_key, sfh_value \n",
    "                        from sparkfits_fits_headers\n",
    "                        where sfh_fits_file='{}' \n",
    "                        order by sfh_index\n",
    "                    ) a\n",
    "                    distribute by grp\n",
    "                    sort by grp, sfh_index\n",
    "                )\n",
    "                select \n",
    "                    grp,\n",
    "                    collect_list(string(a.kva['key']))as keys\n",
    "                    ,collect_list(string(a.kvi['value']))as values\n",
    "                from rawData a\n",
    "                group by grp            \n",
    "            \"\"\".format(fitsFilename)).select(\"keys\",\"values\")\n",
    "            \n",
    "            compress3 = compress2.crossJoin(originalHeader)\n",
    "            test1 = udf(CreateFITSSubCubeUDF, StringType() )\n",
    "            \n",
    "            \n",
    "            compress3=compress3\\\n",
    "            .withColumn('process_msg', test1(compress3.rightAscension, \\\n",
    "                                           compress3.declination, \\\n",
    "                                           compress3.frequencies, \\\n",
    "                                           compress3.pixels, \\\n",
    "                                           compress3.keys, \\\n",
    "                                           compress3.values, \\\n",
    "                                           f.lit(fitsFilename)))\n",
    "            print(\"compress3 created\")\n",
    "            dfout=compress3.select('bins','process_msg')\n",
    "            n=dfout.count()\n",
    "            \n",
    "            \n",
    "            print(\"dfout created - {} rows\".format(str( n )))\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            date_time = now.strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "            print(date_time)\n",
    "            \n",
    "            j=0\n",
    "            writeTries=10\n",
    "            while running and j <= writeTries:\n",
    "                try:\n",
    "                    dfout.select(F.lit(date_time).alias(\"runDate\"), \"bins\", \"process_msg\" )\\\n",
    "                    .write.mode('append').format('parquet').saveAsTable('df_result')\n",
    "                    print(\"Results written\")\n",
    "                    running=False\n",
    "                except Exception as e:\n",
    "                    if j == writeTries:\n",
    "                        errMsg=(\"Writing results to df_result has failed - {}\".format(str(e)))\n",
    "                        raise(WriteDataframeError(errMsg))\n",
    "                        pass\n",
    "                    print(\"Writing results to df_result has failed - retrying...\")\n",
    "                j+=1\n",
    "\n",
    "                \n",
    "        except WriteDataframeError as e:\n",
    "            print(str(e))\n",
    "            print(\"Retrying ProcessThread...\")\n",
    "        except Exception as e:\n",
    "            ## caused by multi[ple inserts as singlerows\n",
    "            print(\"WARNING ProcessThread FAILURE...retrying...\")\n",
    "            \n",
    "            print(\"Cleaning out the cached dataframes\")\n",
    "            for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "                rdd.unpersist()\n",
    "                pass\n",
    "            spark.sparkContext._jsc.getPersistentRDDs().items()\n",
    "            pass\n",
    "        finally:\n",
    "            if i == runTries:\n",
    "                print(\"ERROR - ProcessThread FAILURE...{}\".format(str(e)))\n",
    "                running=False        \n",
    "        i+=1\n",
    "        pass\n",
    "    \n",
    "    for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "\n",
    "    #writeResults(subCubeDF, 'append', 'parquet', 'collatedImages')\n",
    "    #CubeSize=[hiRa,loRa,hiDec,loDec,loFreq,hiFreq]\n",
    "    print(\"done! with ProcessThread\")\n",
    "    print(\"\")\n",
    "    print(\"================================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "raBucketSize=15\n",
    "decBucketSize=15\n",
    "freqBucketSize=15\n",
    "\n",
    "raHist=np.histogram(raArray, raBucketSize)\n",
    "decHist=np.histogram(np.flip(decArray), decBucketSize)\n",
    "freqHist=np.histogram(spectraArray, freqBucketSize)\n",
    "arr = np.empty((0,6), float)\n",
    "\n",
    "for i in np.arange(raBucketSize):\n",
    "    for j in  np.arange(decBucketSize):\n",
    "        for k in np.arange(freqBucketSize):\n",
    "            \n",
    "            x=np.array([[raHist[1][i], raHist[1][i+1], decHist[1][j], decHist[1][j+1], freqHist[1][k], freqHist[1][k+1] ]])\n",
    "            arr=np.append(arr, x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.37649997e+09,  1.37969874e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.37969874e+09,  1.38289751e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.38289751e+09,  1.38609628e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.38609628e+09,  1.38929505e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.38929505e+09,  1.39249382e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.39249382e+09,  1.39569260e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.39569260e+09,  1.39889137e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.39889137e+09,  1.40209014e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.40209014e+09,  1.40528891e+09],\n",
       "       [ 3.28575867e+02,  3.29201455e+02, -4.83297920e+01,\n",
       "        -4.79116412e+01,  1.40528891e+09,  1.40848768e+09]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pools\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1379698739.2 hiFreq 1382897510.4hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1382897510.4 hiFreq 1386096281.6\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1382897510.4, 1386096281.6]\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1389295052.8 hiFreq 1392493824.0\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1389295052.8, 1392493824.0]\n",
      "374 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1379698739.2, 1382897510.4]\n",
      "374Parameters extracted 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "\n",
      "Ra select range determined\n",
      "374 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1386096281.6 hiFreq 1389295052.8\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1386096281.6, 1389295052.8]\n",
      "374 377 172\n",
      "0.097006624\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1376499968.0 hiFreq 1379698739.2\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1376499968.0, 1379698739.2]\n",
      "374 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Raw RA data extracted\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw RA data extracted\n",
      "Raw RA data extracted\n",
      "Raw RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Declination data extracted\n",
      "Filtered RA data extracted\n",
      "Filtered RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Frequency data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extractedRaw Declination data extracted\n",
      "Raw Declination data extracted\n",
      "\n",
      "Raw Frequency data extracted\n",
      "Raw Frequency data extracted\n",
      "Raw Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Caching Raw image dataCaching Raw image data\n",
      "Caching Raw image data\n",
      "\n",
      "Filtered Dec and Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Caching Raw image data\n",
      "Caching Raw image data\n",
      "Raw Images data extracted\n",
      "Raw Images data extracted\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Caching return subcube...\n",
      "Caching return subcube...\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 106.6s, CPU 1.2s\n",
      "GetSubCube2 created - creating temp table\n",
      "Elapsed 106.6s, CPU 1.2s\n",
      "GetSubCube2 created - creating temp table\n",
      "compress3 created\n",
      "compress3 created\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 107.5s, CPU 1.2s\n",
      "GetSubCube2 created - creating temp table\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 107.8s, CPU 1.2s\n",
      "GetSubCube2 created - creating temp table\n",
      "compress3 created\n",
      "compress3 created\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 108.3s, CPU 1.3s\n",
      "GetSubCube2 created - creating temp table\n",
      "compress3 created\n",
      "dfout created - 0 rows\n",
      "09/01/2021, 23:58:11\n",
      "dfout created - 0 rows\n",
      "09/01/2021, 23:58:12\n",
      "dfout created - 0 rows\n",
      "09/01/2021, 23:58:13\n",
      "dfout created - 0 rows\n",
      "09/01/2021, 23:58:14\n",
      "dfout created - 0 rows\n",
      "09/01/2021, 23:58:18\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1392493824.0 hiFreq 1395692595.2\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1392493824.0, 1395692595.2]\n",
      "374 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extracted\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1395692595.2 hiFreq 1398891366.4\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1395692595.2, 1398891366.4]\n",
      "374 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Filtered Dec and Frequency data extracted\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1398891366.4 hiFreq 1402090137.6\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1398891366.4, 1402090137.6]\n",
      "374 377 172\n",
      "0.097006624\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extracted\n",
      "Caching Raw image data\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1402090137.6 hiFreq 1405288908.8\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1402090137.6, 1405288908.8]\n",
      "374 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Caching Raw image data\n",
      "Filtered Dec and Frequency data extracted\n",
      "Caching Raw image data\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "hiDec -47.911641184488936 loDec -48.32979202270508 hiRa 329.201454671224 loRa 328.57586669921875 loFreq 1405288908.8 hiFreq 1408487680.0\n",
      "Cube size is  [329.201454671224, 328.57586669921875, -47.911641184488936, -48.32979202270508, 1405288908.8, 1408487680.0]\n",
      "374 377 173\n",
      "0.097570616\n",
      "Caching intermediate dataframes...\n",
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extracted\n",
      "Caching Raw image data\n",
      "Filtered Dec and Frequency data extracted\n",
      "Raw Images data extracted\n",
      "Caching Raw image data\n",
      "Caching return subcube...\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 76.0s, CPU 0.7s\n",
      "GetSubCube2 created - creating temp table\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 76.0s, CPU 0.7s\n",
      "GetSubCube2 created - creating temp table\n",
      "compress3 created\n",
      "compress3 created\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 76.4s, CPU 0.7s\n",
      "GetSubCube2 created - creating temp table\n",
      "compress3 created\n",
      "Raw Images data extracted\n",
      "Caching return subcube...\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 84.6s, CPU 0.9s\n",
      "GetSubCube2 created - creating temp table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compress3 created\n",
      "dfout created - 173 rows\n",
      "10/01/2021, 00:01:00\n",
      "dfout created - 173 rows\n",
      "10/01/2021, 00:01:03\n",
      "dfout created - 173 rows\n",
      "10/01/2021, 00:01:15\n",
      "dfout created - 0 rows\n",
      "10/01/2021, 00:01:22\n",
      "Final base image dataframe created...removing intermediate dataframes from cache...\n",
      "Elapsed 113.1s, CPU 0.9s\n",
      "GetSubCube2 created - creating temp table\n",
      "compress3 created\n",
      "dfout created - 173 rows\n",
      "10/01/2021, 00:01:38\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "Results written\n",
      "done! with ProcessThread\n",
      "\n",
      "================================================================================\n",
      "Elapsed 429.3s, CPU 3.1s\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from time import sleep\n",
    "gNumProcs=5\n",
    "#q=Queue.Queue()\n",
    "tpool=ThreadPool(processes=gNumProcs)\n",
    "\n",
    "data=sc.parallelize(arr[0:10,:])\n",
    "\n",
    "masterLoop=data.map(lambda row: [float(row[0]),float(row[1]),float(row[2]),float(row[3]), \\\n",
    "float(row[4]),float(row[5])] )\n",
    "\n",
    "timer.start()\n",
    "print(\"Loading pools\")\n",
    "billy=tpool.map(ProcessThread, [x for x in masterLoop.toLocalIterator()])\n",
    "timer.stop()\n",
    "\n",
    "print(\"All Done\")\n",
    "\n",
    "#def runQuery(sc,<POOL_ID>,query):\n",
    "#    sc.setLocalProperty(\"spark.scheduler.pool\", pool_id)\n",
    "#    .....<your code>\n",
    "#    return df\n",
    "\n",
    "#t1 = threading.thread(target=runQuery,args=(sc,\"1\",<query1>)\n",
    "#t2 = threading.thread(target=runQuery,args=(sc,\"2\",<query2>)\n",
    "\n",
    "# start the threads...\n",
    "#t1.start()\n",
    "#t2.sart()\n",
    "\n",
    "# wait for the threads to complete and get the returned data frames...\n",
    "#df1 = t1.join()\n",
    "#df2 = t2.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiDec=np.flip(decHist[1])[0]\n",
    "loDec=np.flip(decHist[1])[1]\n",
    "hiRa=raHist[1][1]\n",
    "loRa=raHist[1][0]\n",
    "loFreq=freqHist[1][0] #spectraArray[0]\n",
    "hiFreq=freqHist[1][1] #spectraArray[299]\n",
    "\n",
    "depthOfCubes=5\n",
    "\n",
    "CubeSize=[hiRa,loRa,hiDec,loDec,loFreq,hiFreq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 565 260\n",
      "0.3296436\n"
     ]
    }
   ],
   "source": [
    "raPix=len( np.array(np.where(np.logical_and(raArray >= loRa, raArray <= hiRa )))[0] )\n",
    "decPix=len( np.array(np.where(np.logical_and(decArray >= loDec, decArray <= hiDec )))[0] )\n",
    "frqPix=len( np.array(np.where(np.logical_and(spectraArray >= loFreq, spectraArray <= hiFreq )))[0] )\n",
    "print(raPix, decPix, frqPix)\n",
    "\n",
    "size=raPix*decPix*frqPix*32/8e9 # 8e6 = MB, 8e9= GB\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the ntile size\n",
    "\n",
    "The number of frequencies in each row of the final dataframe is determined by the variable depthOfCubes. We divide the total number of frequencies we are extracting by depthOfCubes to get the number of bins we want.\n",
    "\n",
    "Generally speaking, this should mean that depthOfCubes divides exactly into the number of frequencies, frqPix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntileCount=frqPix/depthOfCubes\n",
    "round(ntileCount,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters extracted\n",
      "Ra select range determined\n",
      "Raw RA data extracted\n",
      "Filtered RA data extracted\n",
      "Raw Declination data extracted\n",
      "Raw Frequency data extracted\n",
      "Filtered Dec and Frequency data extracted\n",
      "Raw Images data extracted\n"
     ]
    }
   ],
   "source": [
    "subCubeDF,raHeaderIndex,decHeaderIndex,freqHeaderIndex,naxis1, naxis2, naxis4 \\\n",
    "=GetSubCube2(sqlContext, fitsFilename, decType,spectraType, raArray, raType, decArray, spectraArray, CubeSize, round(ntileCount) )\n",
    "#GetSubCube2(sqlContext, fitsFilename, decType,spectraType, raArray, decArray, spectraArray, CubeSize, cacheTempTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeResults(subCubeDF, 'overwrite', 'parquet', 'collatedImages')\n",
    "# sqlContext.sql(\"\"\"select * from collatedImages\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE!\n",
    "\n",
    "In order to maintain order when we construct the collect_lists, we need to use DISTRIBUTE BY to guarantee that all rowss for a particular frequency are routed to the same reducer; otherwise we can't guarantee that the order will be maintained.\n",
    "\n",
    "See https://stackoverflow.com/questions/45092576/how-to-use-order-by-with-collect-set-operation-in-hive\n",
    "\n",
    "#### REMEMBER!\n",
    "\n",
    "It's not a relational database where the ordering in usually implicit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE!\n",
    "\n",
    "Drop the temporary tables when you've finished with them to free up resources. \n",
    "\n",
    "For temporary views spark.catalog.dropTempView(\"df\") - these are the session scoped tables, ie. the ones we use in this notebook.\n",
    "\n",
    "For global views spark.catalog.dropGlobalTempView(\"df\")\n",
    "\n",
    "We can also free up resources for dataframes we have finished using, i.e. df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress1=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select bins, sda_Frequency_hz, spi_index, ra as rightAscension,\n",
    "    map(\n",
    "        \n",
    "        'dec', sda_declination\n",
    "    ) as kva,\n",
    "    map(\n",
    "        'pixels', raSelectRange\n",
    "    ) as kvi\n",
    "    from collatedImages\n",
    "    distribute by sda_Frequency_hz\n",
    "    sort by sda_Frequency_hz, spi_index\n",
    ")\n",
    "select \n",
    "    sda_Frequency_hz as frequency,rightAscension,\n",
    "    bins,\n",
    "    collect_list(float(a.kva['dec']))as declination\n",
    "    ,collect_list(array(a.kvi['pixels']))as pixs\n",
    "from rawData a\n",
    "group by frequency, rightAscension, bins\n",
    "\"\"\")\n",
    "\n",
    "if False:\n",
    "    compress1.show()\n",
    "\n",
    "writeResults(compress1, 'overwrite', 'parquet', 'compress_one')\n",
    "sqlContext.sql(\"\"\"drop table collatedImages\"\"\")\n",
    "#sqlContext.sql(\"\"\"select * from compress_one\"\"\").show()\n",
    "#compress1.createOrReplaceTempView(\"compress_one\")\n",
    "if cacheTempTables:\n",
    "    print(\"caching...\")\n",
    "    compress1.cache().count()\n",
    "    spark.catalog.cacheTable(\"compress_one\") \n",
    "    \n",
    "    # No longer need collated images, so unpersist\n",
    "\n",
    "    subCubeDF.unpersist()\n",
    "    spark.catalog.uncacheTable(\"collatedImages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's take it that one step further and compress those three frequencies into one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress2=sqlContext.sql(\"\"\"\n",
    "select\n",
    "    bins,\n",
    "    rightAscension, declination,\n",
    "    collect_list(array(float(a.kvi['frequencies']))) as frequencies,\n",
    "    collect_list(array(a.kva['pixs'])) as pixels\n",
    "from (\n",
    "    select\n",
    "        bins,\n",
    "        rightAscension, declination,\n",
    "        map('frequencies', frequency) as kvi,\n",
    "        map('pixs', pixs) as kva\n",
    "    from compress_one\n",
    "    distribute by rightAscension\n",
    "    sort by frequency\n",
    ") a\n",
    "group by bins,\n",
    "rightAscension, declination\n",
    "\"\"\")\n",
    "\n",
    "writeResults(compress2, 'overwrite', 'parquet', 'compress_two')\n",
    "sqlContext.sql(\"\"\"drop table compress_one\"\"\")\n",
    "\n",
    "if cacheTempTables:\n",
    "    print(\"caching...\")\n",
    "    compress2.cache().count()\n",
    "\n",
    "    # finished with compress1, unpersist\n",
    "    compress1.unpersist()\n",
    "    spark.catalog.uncacheTable(\"compress_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create the new header as a dictionary object\n",
    "\n",
    "So we can pass this in to the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalHeader=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select grp, sfh_index, \n",
    "    map(\n",
    "        \n",
    "        'key', sfh_key\n",
    "    ) as kva,\n",
    "    map(\n",
    "        'value', sfh_value\n",
    "    ) as kvi\n",
    "    from (\n",
    "    --headers\n",
    "        select 1 as grp, sfh_index, sfh_key, sfh_value \n",
    "        from sparkfits_fits_headers\n",
    "        where sfh_fits_file='{}' \n",
    "        order by sfh_index\n",
    "    ) a\n",
    "    distribute by grp\n",
    "    sort by grp, sfh_index\n",
    ")\n",
    "select \n",
    "    grp,\n",
    "    collect_list(string(a.kva['key']))as keys\n",
    "    ,collect_list(string(a.kvi['value']))as values\n",
    "from rawData a\n",
    "group by grp\n",
    "\"\"\".format(fitsFilename)).select(\"keys\",\"values\")\n",
    "\n",
    "# originalHeader.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2=sqlContext.sql(\"select * from compress_two\")\n",
    "compress3 = c2.crossJoin(originalHeader)\n",
    "writeResults(compress3, 'overwrite', 'parquet', 'compress_three')\n",
    "#sqlContext.sql(\"\"\"select * from compress_three\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ERROR! 'Table or view not found: collatedImages;'\n",
      " ERROR! 'Table or view not found: compress_one;'\n"
     ]
    }
   ],
   "source": [
    "for tName in (\"collatedImages\", \"compress_one\", \"compress_two\"):\n",
    "    try:\n",
    "        sqlContext.sql(\"drop table {}\".format(tName))\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        msg = \" ERROR! \"\n",
    "        if hasattr(e, 'message'):\n",
    "            msg += str(e.message)\n",
    "        else:\n",
    "\n",
    "            msg += str(e)\n",
    "            pass\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cacheTempTables:\n",
    "    compress3.cache().count()\n",
    "    compress2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'CartesianProduct\\n:- InMemoryTableScan [sda_index#1097, sda_declination#1098]\\n:     +- InMemoryRelation [sda_index#1097, sda_declination#1098], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n:           +- Filter ((isnotnull(sda_declination#1098) && (cast(sda_declination#1098 as double) <= -42.05752944946289)) && (cast(sda_declination#1098 as double) >= -43.311981964111325))\\n:              +- InMemoryTableScan [sda_index#1097, sda_declination#1098], [isnotnull(sda_declination#1098), (cast(sda_declination#1098 as double) <= -42.05752944946289), (cast(sda_declination#1098 as double) >= -43.311981964111325)]\\n:                    +- InMemoryRelation [sda_index#1097, sda_declination#1098], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n:                          +- Project [pos#125 AS sda_index#120, col#126 AS sda_declination#121]\\n:                             +- Generate posexplode(sda_detail_array#122), false, [pos#125, col#126]\\n:                                +- Project [sda_detail_array#122]\\n:    ...',\n",
       "  'memSize_MB': 0.0,\n",
       "  'memSize_GB': 0.0,\n",
       "  'diskSize_MB': 0.7364749908447266,\n",
       "  'diskSize_GB': 0.0007192138582468033,\n",
       "  'numPartitions': 768,\n",
       "  'numCachedPartitions': 768,\n",
       "  'callSite': 'persist at <unknown>:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 288,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5484,\n",
       "  'scope': JavaObject id=o5485,\n",
       "  'storageLevel': JavaObject id=o5486,\n",
       "  'toString': 'RDD \"CartesianProduct\\n:- InMemoryTableScan [sda_index#1097, sda_declination#1098]\\n:     +- InMemoryRelation [sda_index#1097, sda_declination#1098], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n:           +- Filter ((isnotnull(sda_declination#1098) && (cast(sda_declination#1098 as double) <= -42.05752944946289)) && (cast(sda_declination#1098 as double) >= -43.311981964111325))\\n:              +- InMemoryTableScan [sda_index#1097, sda_declination#1098], [isnotnull(sda_declination#1098), (cast(sda_declination#1098 as double) <= -42.05752944946289), (cast(sda_declination#1098 as double) >= -43.311981964111325)]\\n:                    +- InMemoryRelation [sda_index#1097, sda_declination#1098], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n:                          +- Project [pos#125 AS sda_index#120, col#126 AS sda_declination#121]\\n:                             +- Generate posexplode(sda_detail_array#122), false, [pos#125, col#126]\\n:                                +- Project [sda_detail_array#122]\\n:    ...\" (288) StorageLevel: StorageLevel(disk, 1 replicas); CachedPartitions: 768; TotalPartitions: 768; MemorySize: 0.0 B; DiskSize: 754.2 KB'},\n",
       " {'name': 'Project [spi_index#1295, ra#1068, sda_declination#1098, spi_image#1296, spi_filename#1297, spi_band#1298, sda_Frequency_hz#1107, bins#1142]\\n+- BroadcastHashJoin [spi_index#1295], [sda_index#1097], Inner, BuildRight\\n   :- Project [spi_index#1295, spi_image#1296, spi_filename#1297, spi_band#1298, sda_frequency_hz#1107, bins#1142]\\n   :  +- BroadcastHashJoin [spi_band#1298], [sda_index#1106], Inner, BuildRight\\n   :     :- Filter (isnotnull(spi_band#1298) && isnotnull(spi_index#1295))\\n   :     :  +- InMemoryTableScan [spi_index#1295, spi_image#1296, spi_filename#1297, spi_band#1298], [isnotnull(spi_band#1298), isnotnull(spi_index#1295)]\\n   :     :        +- InMemoryRelation [spi_index#1295, spi_image#1296, spi_filename#1297, spi_band#1298], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n   :     :              +- Sort [spi_band#1298 ASC NULLS FIRST, spi_index#1295 ASC NULLS FIRST], false, 0\\n   :     :                 +- Exchange hashpartitioning(spi_band#1298, 768)\\n   :     :                    +- FileScan...',\n",
       "  'memSize_MB': 8609.936444282532,\n",
       "  'memSize_GB': 8.40814105886966,\n",
       "  'diskSize_MB': 0.0,\n",
       "  'diskSize_GB': 0.0,\n",
       "  'numPartitions': 768,\n",
       "  'numCachedPartitions': 547,\n",
       "  'callSite': 'persist at <unknown>:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 343,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5488,\n",
       "  'scope': JavaObject id=o5489,\n",
       "  'storageLevel': JavaObject id=o5490,\n",
       "  'toString': 'RDD \"Project [spi_index#1295, ra#1068, sda_declination#1098, spi_image#1296, spi_filename#1297, spi_band#1298, sda_Frequency_hz#1107, bins#1142]\\n+- BroadcastHashJoin [spi_index#1295], [sda_index#1097], Inner, BuildRight\\n   :- Project [spi_index#1295, spi_image#1296, spi_filename#1297, spi_band#1298, sda_frequency_hz#1107, bins#1142]\\n   :  +- BroadcastHashJoin [spi_band#1298], [sda_index#1106], Inner, BuildRight\\n   :     :- Filter (isnotnull(spi_band#1298) && isnotnull(spi_index#1295))\\n   :     :  +- InMemoryTableScan [spi_index#1295, spi_image#1296, spi_filename#1297, spi_band#1298], [isnotnull(spi_band#1298), isnotnull(spi_index#1295)]\\n   :     :        +- InMemoryRelation [spi_index#1295, spi_image#1296, spi_filename#1297, spi_band#1298], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n   :     :              +- Sort [spi_band#1298 ASC NULLS FIRST, spi_index#1295 ASC NULLS FIRST], false, 0\\n   :     :                 +- Exchange hashpartitioning(spi_band#1298, 768)\\n   :     :                    +- FileScan...\" (343) StorageLevel: StorageLevel(memory, 1 replicas); CachedPartitions: 547; TotalPartitions: 768; MemorySize: 8.4 GB; DiskSize: 0.0 B'},\n",
       " {'name': 'Window [ntile(104) windowspecdefinition(sda_index#1106 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS bins#1142], [sda_index#1106 ASC NULLS FIRST]\\n+- Sort [sda_index#1106 ASC NULLS FIRST], false, 0\\n   +- Exchange SinglePartition\\n      +- Filter ((isnotnull(sda_Frequency_hz#1107) && (cast(sda_Frequency_hz#1107 as double) <= 1.3860962816E9)) && (cast(sda_Frequency_hz#1107 as double) >= 1.376499968E9))\\n         +- InMemoryTableScan [sda_index#1106, sda_Frequency_hz#1107], [isnotnull(sda_Frequency_hz#1107), (cast(sda_Frequency_hz#1107 as double) <= 1.3860962816E9), (cast(sda_Frequency_hz#1107 as double) >= 1.376499968E9)]\\n               +- InMemoryRelation [sda_index#1106, sda_Frequency_hz#1107], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n                     +- Project [pos#144 AS sda_index#139, col#145 AS sda_Frequency_hz#140]\\n                        +- Generate posexplode(sda_detail_array#141), false, [pos#144, col#145]\\n                           +- Projec...',\n",
       "  'memSize_MB': 0.002231597900390625,\n",
       "  'memSize_GB': 2.1792948246002197e-06,\n",
       "  'diskSize_MB': 0.0,\n",
       "  'diskSize_GB': 0.0,\n",
       "  'numPartitions': 1,\n",
       "  'numCachedPartitions': 1,\n",
       "  'callSite': 'persist at <unknown>:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 279,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5492,\n",
       "  'scope': JavaObject id=o5493,\n",
       "  'storageLevel': JavaObject id=o5494,\n",
       "  'toString': 'RDD \"Window [ntile(104) windowspecdefinition(sda_index#1106 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS bins#1142], [sda_index#1106 ASC NULLS FIRST]\\n+- Sort [sda_index#1106 ASC NULLS FIRST], false, 0\\n   +- Exchange SinglePartition\\n      +- Filter ((isnotnull(sda_Frequency_hz#1107) && (cast(sda_Frequency_hz#1107 as double) <= 1.3860962816E9)) && (cast(sda_Frequency_hz#1107 as double) >= 1.376499968E9))\\n         +- InMemoryTableScan [sda_index#1106, sda_Frequency_hz#1107], [isnotnull(sda_Frequency_hz#1107), (cast(sda_Frequency_hz#1107 as double) <= 1.3860962816E9), (cast(sda_Frequency_hz#1107 as double) >= 1.376499968E9)]\\n               +- InMemoryRelation [sda_index#1106, sda_Frequency_hz#1107], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n                     +- Project [pos#144 AS sda_index#139, col#145 AS sda_Frequency_hz#140]\\n                        +- Generate posexplode(sda_detail_array#141), false, [pos#144, col#145]\\n                           +- Projec...\" (279) StorageLevel: StorageLevel(memory, 1 replicas); CachedPartitions: 1; TotalPartitions: 1; MemorySize: 2.3 KB; DiskSize: 0.0 B'},\n",
       " {'name': 'Sort [spi_band#1298 ASC NULLS FIRST, spi_index#1295 ASC NULLS FIRST], false, 0\\n+- Exchange hashpartitioning(spi_band#1298, 768)\\n   +- FileScan parquet fits_investigation.sparkfits_images[spi_index#1295,spi_image#1296,spi_filename#1297,spi_band#1298] Batched: false, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/fits_invest..., PartitionCount: 519, PartitionFilters: [isnotnull(spi_filename#1297), isnotnull(spi_band#1298), (spi_filename#1297 = image.restored.i.SB..., PushedFilters: [], ReadSchema: struct<spi_index:int,spi_image:array<float>>\\n',\n",
       "  'memSize_MB': 45047.1128282547,\n",
       "  'memSize_GB': 43.99132112134248,\n",
       "  'diskSize_MB': 784.2140607833862,\n",
       "  'diskSize_GB': 0.7658340437337756,\n",
       "  'numPartitions': 768,\n",
       "  'numCachedPartitions': 547,\n",
       "  'callSite': 'persist at <unknown>:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 326,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5496,\n",
       "  'scope': JavaObject id=o5497,\n",
       "  'storageLevel': JavaObject id=o5498,\n",
       "  'toString': 'RDD \"Sort [spi_band#1298 ASC NULLS FIRST, spi_index#1295 ASC NULLS FIRST], false, 0\\n+- Exchange hashpartitioning(spi_band#1298, 768)\\n   +- FileScan parquet fits_investigation.sparkfits_images[spi_index#1295,spi_image#1296,spi_filename#1297,spi_band#1298] Batched: false, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/fits_invest..., PartitionCount: 519, PartitionFilters: [isnotnull(spi_filename#1297), isnotnull(spi_band#1298), (spi_filename#1297 = image.restored.i.SB..., PushedFilters: [], ReadSchema: struct<spi_index:int,spi_image:array<float>>\\n\" (326) StorageLevel: StorageLevel(memory, 1 replicas); CachedPartitions: 547; TotalPartitions: 768; MemorySize: 44.0 GB; DiskSize: 784.2 MB'},\n",
       " {'name': 'Project [bins#1142, spi_index#1295, ra#1068, sda_declination#1098, spi_image#1296, array(spi_image#1296[4485], spi_image#1296[4486], spi_image#1296[4487], spi_image#1296[4488], spi_image#1296[4489], spi_image#1296[4490], spi_image#1296[4491], spi_image#1296[4492], spi_image#1296[4493], spi_image#1296[4494], spi_image#1296[4495], spi_image#1296[4496], spi_image#1296[4497], spi_image#1296[4498], spi_image#1296[4499], spi_image#1296[4500], spi_image#1296[4501], spi_image#1296[4502], spi_image#1296[4503], spi_image#1296[4504], spi_image#1296[4505], spi_image#1296[4506], spi_image#1296[4507], spi_image#1296[4508], ... 1098 more fields) AS raSelectRange#1426, spi_filename#1297, spi_band#1298, sda_Frequency_hz#1107]\\n+- InMemoryTableScan [bins#1142, ra#1068, sda_Frequency_hz#1107, sda_declination#1098, spi_band#1298, spi_filename#1297, spi_image#1296, spi_index#1295]\\n      +- InMemoryRelation [spi_index#1295, ra#1068, sda_declination#1098, spi_image#1296, spi_filename#1297, spi_band#1298, sda_Frequency_hz#1107, bi...',\n",
       "  'memSize_MB': 10070.51658821106,\n",
       "  'memSize_GB': 9.834488855674863,\n",
       "  'diskSize_MB': 0.0,\n",
       "  'diskSize_GB': 0.0,\n",
       "  'numPartitions': 768,\n",
       "  'numCachedPartitions': 547,\n",
       "  'callSite': 'persist at <unknown>:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 348,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5500,\n",
       "  'scope': JavaObject id=o5501,\n",
       "  'storageLevel': JavaObject id=o5502,\n",
       "  'toString': 'RDD \"Project [bins#1142, spi_index#1295, ra#1068, sda_declination#1098, spi_image#1296, array(spi_image#1296[4485], spi_image#1296[4486], spi_image#1296[4487], spi_image#1296[4488], spi_image#1296[4489], spi_image#1296[4490], spi_image#1296[4491], spi_image#1296[4492], spi_image#1296[4493], spi_image#1296[4494], spi_image#1296[4495], spi_image#1296[4496], spi_image#1296[4497], spi_image#1296[4498], spi_image#1296[4499], spi_image#1296[4500], spi_image#1296[4501], spi_image#1296[4502], spi_image#1296[4503], spi_image#1296[4504], spi_image#1296[4505], spi_image#1296[4506], spi_image#1296[4507], spi_image#1296[4508], ... 1098 more fields) AS raSelectRange#1426, spi_filename#1297, spi_band#1298, sda_Frequency_hz#1107]\\n+- InMemoryTableScan [bins#1142, ra#1068, sda_Frequency_hz#1107, sda_declination#1098, spi_band#1298, spi_filename#1297, spi_image#1296, spi_index#1295]\\n      +- InMemoryRelation [spi_index#1295, ra#1068, sda_declination#1098, spi_image#1296, spi_filename#1297, spi_band#1298, sda_Frequency_hz#1107, bi...\" (348) StorageLevel: StorageLevel(memory, 1 replicas); CachedPartitions: 547; TotalPartitions: 768; MemorySize: 9.8 GB; DiskSize: 0.0 B'},\n",
       " {'name': 'Project [pos#1053 AS sda_index#1048, col#1054 AS sda_ra#1049]\\n+- Filter ((isnotnull(col#1054) && (cast(col#1054 as double) <= 330.4526306152344)) && (cast(col#1054 as double) >= 328.57586669921875))\\n   +- Generate posexplode(sda_detail_array#1050), false, [pos#1053, col#1054]\\n      +- Project [sda_detail_array#1050]\\n         +- FileScan parquet fits_investigation.sparkfits_detail_arrays[sda_detail_array#1050,sda_filename#1051,sda_detail_type#1052] Batched: false, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/fits_invest..., PartitionCount: 1, PartitionFilters: [isnotnull(sda_filename#1051), isnotnull(sda_detail_type#1052), (sda_filename#1051 = image.restor..., PushedFilters: [], ReadSchema: struct<sda_detail_array:array<float>>\\n',\n",
       "  'memSize_MB': 0.004523277282714844,\n",
       "  'memSize_GB': 4.417262971401215e-06,\n",
       "  'diskSize_MB': 0.0,\n",
       "  'diskSize_GB': 0.0,\n",
       "  'numPartitions': 1,\n",
       "  'numCachedPartitions': 1,\n",
       "  'callSite': 'persist at NativeMethodAccessorImpl.java:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 256,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5504,\n",
       "  'scope': JavaObject id=o5505,\n",
       "  'storageLevel': JavaObject id=o5506,\n",
       "  'toString': 'RDD \"Project [pos#1053 AS sda_index#1048, col#1054 AS sda_ra#1049]\\n+- Filter ((isnotnull(col#1054) && (cast(col#1054 as double) <= 330.4526306152344)) && (cast(col#1054 as double) >= 328.57586669921875))\\n   +- Generate posexplode(sda_detail_array#1050), false, [pos#1053, col#1054]\\n      +- Project [sda_detail_array#1050]\\n         +- FileScan parquet fits_investigation.sparkfits_detail_arrays[sda_detail_array#1050,sda_filename#1051,sda_detail_type#1052] Batched: false, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/fits_invest..., PartitionCount: 1, PartitionFilters: [isnotnull(sda_filename#1051), isnotnull(sda_detail_type#1052), (sda_filename#1051 = image.restor..., PushedFilters: [], ReadSchema: struct<sda_detail_array:array<float>>\\n\" (256) StorageLevel: StorageLevel(memory, 1 replicas); CachedPartitions: 1; TotalPartitions: 1; MemorySize: 4.6 KB; DiskSize: 0.0 B'},\n",
       " {'name': 'Filter ((isnotnull(sda_declination#1098) && (cast(sda_declination#1098 as double) <= -42.05752944946289)) && (cast(sda_declination#1098 as double) >= -43.311981964111325))\\n+- InMemoryTableScan [sda_index#1097, sda_declination#1098], [isnotnull(sda_declination#1098), (cast(sda_declination#1098 as double) <= -42.05752944946289), (cast(sda_declination#1098 as double) >= -43.311981964111325)]\\n      +- InMemoryRelation [sda_index#1097, sda_declination#1098], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n            +- Project [pos#125 AS sda_index#120, col#126 AS sda_declination#121]\\n               +- Generate posexplode(sda_detail_array#122), false, [pos#125, col#126]\\n                  +- Project [sda_detail_array#122]\\n                     +- FileScan parquet fits_investigation.sparkfits_detail_arrays[sda_detail_array#122,sda_filename#123,sda_detail_type#124] Batched: false, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/fits_invest..., Pa...',\n",
       "  'memSize_MB': 0.0,\n",
       "  'memSize_GB': 0.0,\n",
       "  'diskSize_MB': 0.004555702209472656,\n",
       "  'diskSize_GB': 4.448927938938141e-06,\n",
       "  'numPartitions': 1,\n",
       "  'numCachedPartitions': 1,\n",
       "  'callSite': 'persist at NativeMethodAccessorImpl.java:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 270,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5508,\n",
       "  'scope': JavaObject id=o5509,\n",
       "  'storageLevel': JavaObject id=o5510,\n",
       "  'toString': 'RDD \"Filter ((isnotnull(sda_declination#1098) && (cast(sda_declination#1098 as double) <= -42.05752944946289)) && (cast(sda_declination#1098 as double) >= -43.311981964111325))\\n+- InMemoryTableScan [sda_index#1097, sda_declination#1098], [isnotnull(sda_declination#1098), (cast(sda_declination#1098 as double) <= -42.05752944946289), (cast(sda_declination#1098 as double) >= -43.311981964111325)]\\n      +- InMemoryRelation [sda_index#1097, sda_declination#1098], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n            +- Project [pos#125 AS sda_index#120, col#126 AS sda_declination#121]\\n               +- Generate posexplode(sda_detail_array#122), false, [pos#125, col#126]\\n                  +- Project [sda_detail_array#122]\\n                     +- FileScan parquet fits_investigation.sparkfits_detail_arrays[sda_detail_array#122,sda_filename#123,sda_detail_type#124] Batched: false, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/fits_invest..., Pa...\" (270) StorageLevel: StorageLevel(disk, 1 replicas); CachedPartitions: 1; TotalPartitions: 1; MemorySize: 0.0 B; DiskSize: 4.7 KB'},\n",
       " {'name': 'ObjectHashAggregate(keys=[1#1084], functions=[collect_list(kv#1070[sda_ra], 0, 0)], output=[grp#1069, ra#1068])\\n+- Exchange hashpartitioning(1#1084, 768)\\n   +- ObjectHashAggregate(keys=[1 AS 1#1084], functions=[partial_collect_list(kv#1070[sda_ra], 0, 0)], output=[1#1084, buf#1086])\\n      +- Project [map(sda_ra, sda_ra#1049) AS kv#1070]\\n         +- InMemoryTableScan [sda_ra#1049]\\n               +- InMemoryRelation [sda_index#1048, sda_ra#1049], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n                     +- Project [pos#1053 AS sda_index#1048, col#1054 AS sda_ra#1049]\\n                        +- Filter ((isnotnull(col#1054) && (cast(col#1054 as double) <= 330.4526306152344)) && (cast(col#1054 as double) >= 328.57586669921875))\\n                           +- Generate posexplode(sda_detail_array#1050), false, [pos#1053, col#1054]\\n                              +- Project [sda_detail_array#1050]\\n                                 +- FileScan parquet fits_investigation.sparkfits_detail_arrays[sda_detail...',\n",
       "  'memSize_MB': 0.0,\n",
       "  'memSize_GB': 0.0,\n",
       "  'diskSize_MB': 0.019865036010742188,\n",
       "  'diskSize_GB': 1.9399449229240417e-05,\n",
       "  'numPartitions': 768,\n",
       "  'numCachedPartitions': 768,\n",
       "  'callSite': 'persist at NativeMethodAccessorImpl.java:0',\n",
       "  'externalBlockStoreSize': 0,\n",
       "  'id': 265,\n",
       "  'isCached': True,\n",
       "  'parentIds': JavaObject id=o5512,\n",
       "  'scope': JavaObject id=o5513,\n",
       "  'storageLevel': JavaObject id=o5514,\n",
       "  'toString': 'RDD \"ObjectHashAggregate(keys=[1#1084], functions=[collect_list(kv#1070[sda_ra], 0, 0)], output=[grp#1069, ra#1068])\\n+- Exchange hashpartitioning(1#1084, 768)\\n   +- ObjectHashAggregate(keys=[1 AS 1#1084], functions=[partial_collect_list(kv#1070[sda_ra], 0, 0)], output=[1#1084, buf#1086])\\n      +- Project [map(sda_ra, sda_ra#1049) AS kv#1070]\\n         +- InMemoryTableScan [sda_ra#1049]\\n               +- InMemoryRelation [sda_index#1048, sda_ra#1049], true, 10000, StorageLevel(disk, memory, 1 replicas)\\n                     +- Project [pos#1053 AS sda_index#1048, col#1054 AS sda_ra#1049]\\n                        +- Filter ((isnotnull(col#1054) && (cast(col#1054 as double) <= 330.4526306152344)) && (cast(col#1054 as double) >= 328.57586669921875))\\n                           +- Generate posexplode(sda_detail_array#1050), false, [pos#1053, col#1054]\\n                              +- Project [sda_detail_array#1050]\\n                                 +- FileScan parquet fits_investigation.sparkfits_detail_arrays[sda_detail...\" (265) StorageLevel: StorageLevel(disk, 1 replicas); CachedPartitions: 768; TotalPartitions: 768; MemorySize: 0.0 B; DiskSize: 20.3 KB'}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\n",
    "    \"name\": s.name(),     \n",
    "    \"memSize_MB\": float(s.memSize())/ 2**20 , \n",
    "    \"memSize_GB\": float(s.memSize())/ 2**30, \n",
    "    \"diskSize_MB\": float(s.diskSize())/ 2**20, \n",
    "    \"diskSize_GB\": float(s.diskSize())/ 2**30, \n",
    "    \"numPartitions\": s.numPartitions(), \n",
    "    \"numCachedPartitions\": s.numCachedPartitions(),\n",
    "    \"callSite\": s.callSite(),\n",
    "    \"externalBlockStoreSize\": s.externalBlockStoreSize(),\n",
    "    \"id\": s.id(),\n",
    "    \"isCached\": s.isCached(),\n",
    "    \"parentIds\": s.parentIds(),\n",
    "    \"scope\": s.scope(),\n",
    "    \"storageLevel\": s.storageLevel(),\n",
    "    \"toString\": s.toString()\n",
    "} for s in sc._jsc.sc().getRDDStorageInfo()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning out the cached dataframes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ItemsView({})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cleaning out the cached dataframes\")\n",
    "for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "    rdd.unpersist()\n",
    "    pass\n",
    "spark.sparkContext._jsc.getPersistentRDDs().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bins: integer (nullable = true)\n",
      " |-- rightAscension: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- declination: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- frequencies: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- pixels: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: float (containsNull = true)\n",
      " |-- keys: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compress3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, StructType,StructField\n",
    "\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"msgNum\", IntegerType(), False),\n",
    "    StructField(\"message\", StringType(), False)\n",
    "]))\n",
    "\n",
    "schema1 = StructType([\n",
    "    \n",
    "    StructField(\"msgNum\", IntegerType(), False),\n",
    "    StructField(\"message\", StringType(), False),\n",
    "])\n",
    "\n",
    "test = udf(CreateFITSSubCubeUDF, ArrayType(IntegerType() ) )\n",
    "test1 = udf(CreateFITSSubCubeUDF, StringType() ) \n",
    "test2 = udf(CreateFITSSubCubeUDF, schema ) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "compress3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfSize=GetDataframeSize(sc,compress3)\n",
    "sc3=compress3.count()\n",
    "\n",
    "print(dfSize, sc3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# .filter(compress3.bins <= 30)\n",
    "dfout=compress3\\\n",
    "    .withColumn('process_msg', test1(compress3.rightAscension, \\\n",
    "                                           compress3.declination, \\\n",
    "                                           compress3.frequencies, \\\n",
    "                                           compress3.pixels, \\\n",
    "                                           compress3.keys, \\\n",
    "                                           compress3.values, \\\n",
    "                                           f.lit(fitsFilename))).select('bins','process_msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout=sqlContext.sql(\"select * from compress_three \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout=dfout\\\n",
    ".withColumn('process_msg', test1(dfout.rightAscension, \\\n",
    "                                           dfout.declination, \\\n",
    "                                           dfout.frequencies, \\\n",
    "                                           dfout.pixels, \\\n",
    "                                           dfout.keys, \\\n",
    "                                           dfout.values, \\\n",
    "                                           f.lit(fitsFilename))).select('bins','process_msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bins: integer (nullable = true)\n",
      " |-- process_msg: string (nullable = true)\n",
      "\n",
      "1943.73456\n"
     ]
    }
   ],
   "source": [
    "dfout.printSchema()\n",
    "dfSize=GetDataframeSize(sc,dfout)\n",
    "\n",
    "print(dfSize)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfout.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28/11/2020, 22:41:00'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now() \n",
    "date_time = now.strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results written\n",
      "Cleaning out the cached dataframes\n",
      "Elapsed 24.3s, CPU 0.0s\n"
     ]
    }
   ],
   "source": [
    "timer.start()\n",
    "try:    \n",
    "    #resBins=dfout.cache().count()\n",
    "    #print(\"dfout has calculated, computing {} bins. Writing results...\".format(resBins))\n",
    "    dfout.select(F.lit(date_time).alias(\"runDate\"), \"bins\", \"process_msg\" )\\\n",
    "    .write.insertInto(\"fits_investigation.df_result\", False)\n",
    "    \n",
    "    print(\"Results written\")\n",
    "    \n",
    "except Exception as e:\n",
    "    msg = \" ERROR! \"\n",
    "    if hasattr(e, 'message'):\n",
    "        msg += str(e.message)\n",
    "    else:\n",
    "        \n",
    "        msg += str(e)\n",
    "        pass\n",
    "    print(msg)\n",
    "\n",
    "finally:\n",
    "    print(\"Cleaning out the cached dataframes\")\n",
    "    for (id, rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "        pass\n",
    "    spark.sparkContext._jsc.getPersistentRDDs().items()\n",
    "\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232.5309450626373\n"
     ]
    }
   ],
   "source": [
    "end_elapsed=time()\n",
    "end_cpu=clock()\n",
    "\n",
    "elapsed=end_elapsed - start_elapsed\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging results...\n",
      "results logged\n"
     ]
    }
   ],
   "source": [
    "LogResults=True\n",
    "\n",
    "if cacheTempTables:\n",
    "    cached = \"Cached\"\n",
    "    pass\n",
    "else:\n",
    "    cached = \"Not cached\"\n",
    "    \n",
    "cube_dimensions=[raPix,decPix,frqPix]\n",
    "\n",
    "if LogResults:\n",
    "    print(\"Logging results...\")\n",
    "    Results = Row(\n",
    "        \"runDate\",\n",
    "        \"Fits_filename\",      \n",
    "        \"cube_Dimensions\",\n",
    "        \"Cached\",\n",
    "        \"SizeEstimate_NumPix\",  \n",
    "        \"SizeEstimate_JavaObj\",        \n",
    "        \"total_elapsed\")\n",
    "    result=Results(\n",
    "        date_time,\n",
    "        fitsFilename,\n",
    "        cube_dimensions,\n",
    "        cached,\n",
    "        size,\n",
    "        dfSize,\n",
    "        elapsed)\n",
    "\n",
    "\n",
    "\n",
    "    resultsDF=spark.createDataFrame([result])\n",
    "    MODE='append'\n",
    "    FORMAT='parquet'\n",
    "    TABLE='SoFiADataframeTestJupyter'\n",
    "    \n",
    "    writeResults(resultsDF, MODE, FORMAT, TABLE)\n",
    "    print(\"results logged\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+----------+-------------------+--------------------+------------------+\n",
      "|             runDate|       Fits_filename| cube_Dimensions|    Cached|SizeEstimate_NumPix|SizeEstimate_JavaObj|     total_elapsed|\n",
      "+--------------------+--------------------+----------------+----------+-------------------+--------------------+------------------+\n",
      "|26/11/2020, 22:16:29|image.restored.i....|[597, 677, 1000]|Not cached|           1.616676|         1281.695904|230.01169610023499|\n",
      "|26/11/2020, 21:12:40|image.restored.i....|[597, 677, 1000]|Not cached|           1.616676|         1284.755904| 573.3540256023407|\n",
      "|27/11/2020, 01:31:40|image.restored.i....|[597, 677, 1000]|Not cached|           1.616676|          1136.27072|  725.933874130249|\n",
      "|27/11/2020, 04:26:44|image.restored.i....| [561, 565, 260]|Not cached|          0.3296436|         1110.913216|164.38950848579407|\n",
      "|27/11/2020, 01:25:14|image.restored.i....|[597, 677, 1000]|Not cached|           1.616676|         1204.030304|317.39001727104187|\n",
      "|27/11/2020, 01:46:28|image.restored.i....| [597, 677, 501]|Not cached|        0.809954676|         1123.338624|477.39290380477905|\n",
      "|26/11/2020, 22:41:26|image.restored.i....|[597, 677, 1500]|Not cached|           2.425014|         1293.661632|   257.35049700737|\n",
      "|26/11/2020, 23:32:37|image.restored.i....|[597, 677, 1200]|Not cached|          1.9400112|         1503.552032|4099.6587381362915|\n",
      "|26/11/2020, 23:02:05|image.restored.i....|[597, 677, 1500]|Not cached|           2.425014|           1319.3832| 299.3754105567932|\n",
      "|27/11/2020, 02:33:22|image.restored.i....| [299, 451, 300]|Not cached|          0.1618188|         1245.768608| 89.14819645881653|\n",
      "+--------------------+--------------------+----------------+----------+-------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from {} \".format('SoFiADataframeTestJupyter')).show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"REFRESH TABLE {} \".format('df_result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+\n",
      "|             runDate|bins|         process_msg|\n",
      "+--------------------+----+--------------------+\n",
      "|27/11/2020, 02:00:16|   1|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   2|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   3|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   4|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   5|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   6|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   7|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   8|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|   9|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  10|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  11|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  12|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  13|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  14|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  15|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  16|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  17|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  18|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  19|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  20|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  21|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  22|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  23|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  24|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  25|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  26|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  27|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  28|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  29|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  30|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  31|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  32|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  33|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  34|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  35|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  36|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  37|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  38|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  39|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  40|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  41|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  42|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  43|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  44|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  45|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  46|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  47|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  48|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  49|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  50|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  51|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  52|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  53|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  54|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  55|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  56|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  57|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  58|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  59|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  60|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  61|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  62|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  63|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  64|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  65|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  66|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  67|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  68|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  69|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  70|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  71|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  72|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  73|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  74|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  75|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  76|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  77|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  78|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  79|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  80|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  81|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  82|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  83|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  84|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  85|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  86|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  87|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  88|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  89|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  90|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  91|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  92|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  93|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  94|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  95|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  96|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  97|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  98|All complete, no ...|\n",
      "|27/11/2020, 02:00:16|  99|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 100|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 101|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 102|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 103|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 104|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 105|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 106|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 107|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 108|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 109|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 110|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 111|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 112|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 113|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 114|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 115|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 116|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 117|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 118|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 119|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 120|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 121|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 122|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 123|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 124|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 125|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 126|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 127|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 128|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 129|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 130|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 131|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 132|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 133|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 134|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 135|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 136|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 137|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 138|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 139|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 140|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 141|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 142|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 143|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 144|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 145|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 146|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 147|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 148|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 149|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 150|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 151|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 152|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 153|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 154|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 155|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 156|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 157|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 158|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 159|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 160|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 161|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 162|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 163|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 164|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 165|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 166|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 167|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 168|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 169|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 170|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 171|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 172|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 173|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 174|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 175|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 176|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 177|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 178|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 179|All complete, no ...|\n",
      "|27/11/2020, 02:00:16| 180|All complete, no ...|\n",
      "+--------------------+----+--------------------+\n",
      "only showing top 180 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from {} \".format('df_result')).show(180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done - Not cached run, elapsed 164.38950848579407, size 1110.913216 \n"
     ]
    }
   ],
   "source": [
    "print(\"All done - {} run, elapsed {}, size {} \".format(cached, str(elapsed), str(dfSize) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark SoFiA Testing (Spark 2.3.0, python 3.6, increased resources)",
   "language": "python",
   "name": "sofia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
