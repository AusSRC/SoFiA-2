{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esther Kundin - The Bloomberg presentation\n",
    "\n",
    "https://databricks.com/session/integrating-existing-c-libraries-into-pyspark\n",
    "\n",
    "## From the SWIG tutorial page\n",
    "\n",
    "http://www.swig.org/tutorial.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create your C file\n",
    "\n",
    "/* File : example.c */\n",
    " \n",
    " #include <time.h>\n",
    " double My_variable = 3.0;\n",
    " \n",
    " int fact(int n) {\n",
    "     if (n <= 1) return 1;\n",
    "     else return n*fact(n-1);\n",
    " }\n",
    " \n",
    " int my_mod(int x, int y) {\n",
    "     return (x%y);\n",
    " }\n",
    " \t\n",
    " char *get_time()\n",
    " {\n",
    "     time_t ltime;\n",
    "     time(&ltime);\n",
    "     return ctime(&ltime);\n",
    " }\n",
    " \n",
    " \n",
    "Interface file\n",
    "Now, in order to add these files to your favorite language, you need to write an \"interface file\" which is the input to SWIG. An interface file for these C functions might look like this :\n",
    "\n",
    " /* example.i */\n",
    " \n",
    " %module example\n",
    " %{\n",
    " /* Put header files here or function declarations like below */\n",
    " extern double My_variable;\n",
    " extern int fact(int n);\n",
    " extern int my_mod(int x, int y);\n",
    " extern char *get_time();\n",
    " %}\n",
    " \n",
    " extern double My_variable;\n",
    " extern int fact(int n);\n",
    " extern int my_mod(int x, int y);\n",
    " extern char *get_time();\n",
    " \n",
    "Building a Python module\n",
    "\n",
    "Turning C code into a Python module is also easy. Simply do the following (shown for Irix, see the SWIG Wiki Shared Libraries FAQ page for help with other operating systems):\n",
    " \n",
    " unix % swig -python example.i\n",
    " unix % gcc -c example.c example_wrap.c \\\n",
    "        -I/usr/local/include/python2.7\n",
    " unix % ld -shared example.o example_wrap.o -o _example.so\n",
    " \n",
    "This creates the _example.so file and the Python wrapper file,\n",
    "\n",
    "Now we need to wrap out code into a zip file so it can be shipped to the Spark cluster.\n",
    "\n",
    "  zip example.zip _example.so example.py\n",
    "  \n",
    "Once we have the .zip file, we need top make sure it's available across the cluster. Place the .zip file in a HDFS directory, and then we use the --pyfiles directive of the spark2-submit command (or put it in the config file for Jupyterhub) i.e.\n",
    "\n",
    "  --pyfiles hdfs://user/hduser/libraries/example.zip\n",
    "  \n",
    "May also need to use the --archives directive, i.e.\n",
    "  --archives example.zip#example\n",
    "  --conf 'spark.executor.extraLibraryPath:./example'\n",
    "  \n",
    "Note that you may also need to specify the spark.driver.extraLibraryPath as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency.\n",
    "\n",
    "### Challenges\n",
    "- UDFs are run on a once per row basis\n",
    "- All function objects passed from the driver to workers insude the UDF need to be able to be pickles - but C/C++ modukes are not able to be pickled\n",
    "- Most interfaces can't be pickled\n",
    "- if not, would create on the executor, row by row\n",
    "\n",
    "### Solutions\n",
    "- Do not keep state in your C++ objects\n",
    "- Spark 2.3 - Use Apache Arrow on vectorised UDFs (good for Pandas, but won't help us with C/C++)\n",
    "- Use Python Singletons for state\n",
    "- df.mapPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark-submit directives\n",
    "\n",
    "| Directive Passed | Set To | Purpose |\n",
    "| ---------------------------------------------- | -------------- | ------------------------------------- |\n",
    "| spark.executor.extraLibraryPath | append path where .so deployed to | Ensure C++ lib is loadable |\n",
    "| spark.driver.extraLibraryPath | append path where .so deployed to | Ensure C++ lib is loadable |\n",
    "| --archives | .zip or .tgz file tha has the .so and config files | Distributes the file to all worker locations |\n",
    "| --pyfiles | .py file that has te UDF code | Distribues the UDF to the workers. Other options are to include it directly in the .py that gets called in the spark-submit directive |\n",
    "| spark.executorEnv.ENVIRONMENT_VARIABLE | Env. Var Value | If the UDF needs Env Vars |\n",
    "| spark.yarn.appMasterEnv.ENVIRONMENT_VARIABLE | Env. Var Value | If the driver code needs Env Vars |\n",
    "    \n",
    "## Example spark-submit\n",
    "\n",
    "$ spark2-submit --master yarn --deploy-mode cluster \\  \n",
    "--conf \"spark.executor.extraLibraryPath=<path>:myfolder\" \\  \n",
    "--conf \"spark.driver.extraLibraryPath=<path>:myfolder\" \\   \n",
    "--archives myfolder.zip#myfolder \\  \n",
    "--conf \"spark.executor.MY_ENV=my_environment_variable\" \\  \n",
    "--conf \"spark.yarn.appMasterEnv.MY_DRIVER_ENV=my_driverenvironment_variable\" \\  \n",
    "the_pyspark_program.py &lt; add file params here &gt;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the mapPartitions example\n",
    "\n",
    "Imports the module once per partition, then runs it over row by row on the dataframe rows on that partition.\n",
    "\n",
    "### The Partitioner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partitioner:\n",
    "    def __init__(self):\n",
    "        self.callPerDriverSetup\n",
    "        \n",
    "    def callPerDriverSetup(self):\n",
    "        pass\n",
    "    \n",
    "    def callPerPartitionSetup(self):\n",
    "        sys.path.append('example')                  ### <=== Either append or use the --pyfiles parameter\n",
    "        import example\n",
    "        self.example=example\n",
    "        \n",
    "    def doProcess(self, element):\n",
    "        return self.example.my_mod(element.wire, 7) ### <==== here's the call to the library\n",
    "    \n",
    "    def processPartition(self, partition):\n",
    "        self.callPerPartitionSetup()\n",
    "        for element in partition:\n",
    "            yield self.doProcess(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMod7(val):\n",
    "    sys.path.append('example')\n",
    "    import example\n",
    "    return example.my_mod(val, 7)\n",
    "\n",
    "def main():\n",
    "    calcmod7= udf(calculateMod7, iType() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how we set up the session and call the class and modules once per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ====================================================\n",
    "    # Set up hive and spark contexts\n",
    "    # ====================================================\n",
    "    \n",
    "    sc = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "    \n",
    "\n",
    "    sCtx=sc.sparkContext\n",
    "    sc.sql(\"use fits_investigation\")\n",
    "    hiveContext = HiveContext(sc)\n",
    "    \n",
    "    # ====================================================\n",
    "    # now we get the dataframe\n",
    "    #  ====================================================\n",
    "    \n",
    "    sqlStmt = (\"\"\"\n",
    "        <create the dataframe from the source files>\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        sofiaDF=sc.sql(sqlStmt)\n",
    "\n",
    "    except Exception as e:\n",
    "        errMsg=(\"Creating the data input dataframe has failed - {}\".format(str(e)))\n",
    "        raise(DataframeError(errMsg))\n",
    "        \n",
    "    p = Partitioner()\n",
    "    \n",
    "    rddout = sofiaDF.rdd.mapPartitions(p.processPartition)\n",
    "    \n",
    "    # ...\n",
    "    # ...\n",
    "    # additional code ...\n",
    "    # ...\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    timer=Timer()\n",
    "\n",
    "    start_elapsed=tm.time()\n",
    "    start_cpu=tm.clock()\n",
    "    \n",
    "    timer.start()\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    timer.stop()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas and SparkFits (Spark 2.3.0, python 3.6, increased resources)",
   "language": "python",
   "name": "sparkfits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
